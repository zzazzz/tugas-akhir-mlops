{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31B_VBmGjHq6"
      },
      "source": [
        "# Install Library yang dibutuhkan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "GOBQcwffXb9S",
        "outputId": "9337e918-fc5d-4389-ff12-f1fa0ea6b2d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tfx\n",
            "  Downloading tfx-1.16.0-py3-none-any.whl.metadata (37 kB)\n",
            "Collecting ml-pipelines-sdk==1.16.0 (from tfx)\n",
            "  Downloading ml_pipelines_sdk-1.16.0-py3-none-any.whl.metadata (33 kB)\n",
            "Requirement already satisfied: absl-py<2.0.0,>=0.9 in /usr/local/lib/python3.10/dist-packages (from tfx) (1.4.0)\n",
            "Collecting ml-metadata<1.17.0,>=1.16.0 (from tfx)\n",
            "  Downloading ml_metadata-1.16.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: packaging>=22 in /usr/local/lib/python3.10/dist-packages (from tfx) (24.2)\n",
            "Requirement already satisfied: portpicker<2,>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from tfx) (1.5.2)\n",
            "Requirement already satisfied: protobuf<5,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tfx) (4.25.5)\n",
            "Collecting docker<8,>=7 (from tfx)\n",
            "  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting google-apitools<1,>=0.5 (from tfx)\n",
            "  Downloading google_apitools-0.5.32-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting google-api-python-client<2,>=1.8 (from tfx)\n",
            "  Downloading google_api_python_client-1.12.11-py2.py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: jinja2<4,>=2.7.3 in /usr/local/lib/python3.10/dist-packages (from tfx) (3.1.4)\n",
            "Requirement already satisfied: typing-extensions<5 in /usr/local/lib/python3.10/dist-packages (from tfx) (4.12.2)\n",
            "Collecting apache-beam<3,>=2.47 (from apache-beam[gcp]<3,>=2.47->tfx)\n",
            "  Downloading apache_beam-2.61.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\n",
            "Collecting attrs<24,>=19.3.0 (from tfx)\n",
            "  Downloading attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: click<9,>=7 in /usr/local/lib/python3.10/dist-packages (from tfx) (8.1.7)\n",
            "Requirement already satisfied: google-api-core<3 in /usr/local/lib/python3.10/dist-packages (from tfx) (2.19.2)\n",
            "Requirement already satisfied: google-cloud-aiplatform<2,>=1.6.2 in /usr/local/lib/python3.10/dist-packages (from tfx) (1.74.0)\n",
            "Requirement already satisfied: google-cloud-bigquery<4,>=3 in /usr/local/lib/python3.10/dist-packages (from tfx) (3.25.0)\n",
            "Requirement already satisfied: grpcio<2,>=1.28.1 in /usr/local/lib/python3.10/dist-packages (from tfx) (1.68.1)\n",
            "Collecting keras-tuner!=1.4.0,!=1.4.1,<2,>=1.0.4 (from tfx)\n",
            "  Downloading keras_tuner-1.4.7-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting kubernetes<27,>=10.0.1 (from tfx)\n",
            "  Downloading kubernetes-26.1.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: numpy<2,>=1.16 in /usr/local/lib/python3.10/dist-packages (from tfx) (1.26.4)\n",
            "Collecting pyarrow<11,>=10 (from tfx)\n",
            "  Downloading pyarrow-10.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: orjson!=3.10.7 in /usr/local/lib/python3.10/dist-packages (from tfx) (3.10.12)\n",
            "Collecting scipy<1.13 (from tfx)\n",
            "  Downloading scipy-1.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.4/60.4 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scikit-learn==1.5.1 (from tfx)\n",
            "  Downloading scikit_learn-1.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Requirement already satisfied: pyyaml<7,>=6 in /usr/local/lib/python3.10/dist-packages (from tfx) (6.0.2)\n",
            "Collecting tensorflow<2.17,>=2.16.0 (from tfx)\n",
            "  Downloading tensorflow-2.16.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
            "Collecting tensorflow-hub<0.16,>=0.15.0 (from tfx)\n",
            "  Downloading tensorflow_hub-0.15.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting tensorflow-data-validation<1.17.0,>=1.16.1 (from tfx)\n",
            "  Downloading tensorflow_data_validation-1.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
            "Collecting tensorflow-model-analysis<0.48.0,>=0.47.0 (from tfx)\n",
            "  Downloading tensorflow_model_analysis-0.47.1-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting tensorflow-serving-api<2.17,>=2.16 (from tfx)\n",
            "  Downloading tensorflow_serving_api-2.16.1-py2.py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting tensorflow-transform<1.17.0,>=1.16.0 (from tfx)\n",
            "  Downloading tensorflow_transform-1.16.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting tfx-bsl<1.17.0,>=1.16.1 (from tfx)\n",
            "  Downloading tfx_bsl-1.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.5.1->tfx) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.5.1->tfx) (3.5.0)\n",
            "Collecting crcmod<2.0,>=1.7 (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tfx)\n",
            "  Downloading crcmod-1.7.tar.gz (89 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.7/89.7 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting dill<0.3.2,>=0.3.1.1 (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tfx)\n",
            "  Downloading dill-0.3.1.1.tar.gz (151 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.0/152.0 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting cloudpickle~=2.2.1 (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tfx)\n",
            "  Downloading cloudpickle-2.2.1-py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting fastavro<2,>=0.23.6 (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tfx)\n",
            "  Downloading fastavro-1.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
            "Collecting fasteners<1.0,>=0.3 (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tfx)\n",
            "  Downloading fasteners-0.19-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting grpcio<2,>=1.28.1 (from tfx)\n",
            "  Downloading grpcio-1.65.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.3 kB)\n",
            "Collecting hdfs<3.0.0,>=2.1.0 (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tfx)\n",
            "  Downloading hdfs-2.7.3.tar.gz (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: httplib2<0.23.0,>=0.8 in /usr/local/lib/python3.10/dist-packages (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tfx) (0.22.0)\n",
            "Requirement already satisfied: jsonschema<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tfx) (4.23.0)\n",
            "Collecting jsonpickle<4.0.0,>=3.0.0 (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tfx)\n",
            "  Downloading jsonpickle-3.4.2-py3-none-any.whl.metadata (8.1 kB)\n",
            "Collecting objsize<0.8.0,>=0.6.1 (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tfx)\n",
            "  Downloading objsize-0.7.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting pymongo<5.0.0,>=3.8.0 (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tfx)\n",
            "  Downloading pymongo-4.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\n",
            "Requirement already satisfied: proto-plus<2,>=1.7.1 in /usr/local/lib/python3.10/dist-packages (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tfx) (1.25.0)\n",
            "Collecting pydot<2,>=1.2.0 (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tfx)\n",
            "  Downloading pydot-1.4.2-py2.py3-none-any.whl.metadata (8.0 kB)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.8.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tfx) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2018.3 in /usr/local/lib/python3.10/dist-packages (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tfx) (2024.2)\n",
            "Collecting redis<6,>=5.0.0 (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tfx)\n",
            "  Downloading redis-5.2.1-py3-none-any.whl.metadata (9.1 kB)\n",
            "Requirement already satisfied: regex>=2020.6.8 in /usr/local/lib/python3.10/dist-packages (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tfx) (2024.11.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.24.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tfx) (2.32.3)\n",
            "Collecting sortedcontainers>=2.4.0 (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tfx)\n",
            "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
            "Collecting zstandard<1,>=0.18.0 (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tfx)\n",
            "  Downloading zstandard-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Collecting pyarrow-hotfix<1 (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tfx)\n",
            "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: cachetools<6,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (5.5.0)\n",
            "Collecting google-apitools<1,>=0.5 (from tfx)\n",
            "  Downloading google-apitools-0.5.31.tar.gz (173 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.5/173.5 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: google-auth<3,>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (2.27.0)\n",
            "Requirement already satisfied: google-auth-httplib2<0.3.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (0.2.0)\n",
            "Requirement already satisfied: google-cloud-datastore<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (2.20.2)\n",
            "Requirement already satisfied: google-cloud-pubsub<3,>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (2.27.1)\n",
            "Collecting google-cloud-pubsublite<2,>=1.2.0 (from apache-beam[gcp]<3,>=2.47->tfx)\n",
            "  Downloading google_cloud_pubsublite-1.11.1-py2.py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: google-cloud-storage<3,>=2.18.2 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (2.19.0)\n",
            "Requirement already satisfied: google-cloud-bigquery-storage<3,>=2.6.3 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (2.27.0)\n",
            "Requirement already satisfied: google-cloud-core<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (2.4.1)\n",
            "Requirement already satisfied: google-cloud-bigtable<3,>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (2.27.0)\n",
            "Collecting google-cloud-spanner<4,>=3.0.0 (from apache-beam[gcp]<3,>=2.47->tfx)\n",
            "  Downloading google_cloud_spanner-3.51.0-py2.py3-none-any.whl.metadata (10 kB)\n",
            "Collecting google-cloud-dlp<4,>=3.0.0 (from apache-beam[gcp]<3,>=2.47->tfx)\n",
            "  Downloading google_cloud_dlp-3.26.0-py2.py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: google-cloud-language<3,>=2.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (2.16.0)\n",
            "Collecting google-cloud-videointelligence<3,>=2.0 (from apache-beam[gcp]<3,>=2.47->tfx)\n",
            "  Downloading google_cloud_videointelligence-2.15.0-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting google-cloud-vision<4,>=2 (from apache-beam[gcp]<3,>=2.47->tfx)\n",
            "  Downloading google_cloud_vision-3.9.0-py2.py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting google-cloud-recommendations-ai<0.11.0,>=0.1.0 (from apache-beam[gcp]<3,>=2.47->tfx)\n",
            "  Downloading google_cloud_recommendations_ai-0.10.15-py2.py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting keyrings.google-artifactregistry-auth (from apache-beam[gcp]<3,>=2.47->tfx)\n",
            "  Downloading keyrings.google_artifactregistry_auth-1.1.2-py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from docker<8,>=7->tfx) (2.2.3)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core<3->tfx) (1.66.0)\n",
            "Requirement already satisfied: six<2dev,>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client<2,>=1.8->tfx) (1.17.0)\n",
            "Collecting uritemplate<4dev,>=3.0.0 (from google-api-python-client<2,>=1.8->tfx)\n",
            "  Downloading uritemplate-3.0.1-py2.py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: oauth2client>=1.4.12 in /usr/local/lib/python3.10/dist-packages (from google-apitools<1,>=0.5->tfx) (4.1.3)\n",
            "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2,>=1.6.2->tfx) (1.14.0)\n",
            "Requirement already satisfied: shapely<3.0.0dev in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2,>=1.6.2->tfx) (2.0.6)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2,>=1.6.2->tfx) (2.10.3)\n",
            "Requirement already satisfied: docstring-parser<1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2,>=1.6.2->tfx) (0.16)\n",
            "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery<4,>=3->tfx) (2.7.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2<4,>=2.7.3->tfx) (3.0.2)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (from keras-tuner!=1.4.0,!=1.4.1,<2,>=1.0.4->tfx) (3.5.0)\n",
            "Collecting kt-legacy (from keras-tuner!=1.4.0,!=1.4.1,<2,>=1.0.4->tfx)\n",
            "  Downloading kt_legacy-1.0.5-py3-none-any.whl.metadata (221 bytes)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.10/dist-packages (from kubernetes<27,>=10.0.1->tfx) (2024.12.14)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes<27,>=10.0.1->tfx) (75.1.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes<27,>=10.0.1->tfx) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes<27,>=10.0.1->tfx) (1.3.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from portpicker<2,>=1.3.1->tfx) (5.9.5)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.0->tfx) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.0->tfx) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.0->tfx) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.0->tfx) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.0->tfx) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.0->tfx) (18.1.1)\n",
            "Collecting ml-dtypes~=0.3.1 (from tensorflow<2.17,>=2.16.0->tfx)\n",
            "  Downloading ml_dtypes-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.0->tfx) (3.4.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.0->tfx) (2.5.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.0->tfx) (1.17.0)\n",
            "Collecting tensorboard<2.17,>=2.16 (from tensorflow<2.17,>=2.16.0->tfx)\n",
            "  Downloading tensorboard-2.16.2-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.0->tfx) (0.37.1)\n",
            "Collecting pandas<2,>=1.0 (from tensorflow-data-validation<1.17.0,>=1.16.1->tfx)\n",
            "  Downloading pandas-1.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting pyfarmhash<0.4,>=0.2.2 (from tensorflow-data-validation<1.17.0,>=1.16.1->tfx)\n",
            "  Downloading pyfarmhash-0.3.2.tar.gz (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.9/99.9 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting tensorflow-metadata<1.17,>=1.16.0 (from tensorflow-data-validation<1.17.0,>=1.16.1->tfx)\n",
            "  Downloading tensorflow_metadata-1.16.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: ipython<8,>=7 in /usr/local/lib/python3.10/dist-packages (from tensorflow-model-analysis<0.48.0,>=0.47.0->tfx) (7.34.0)\n",
            "Requirement already satisfied: ipywidgets<8,>=7 in /usr/local/lib/python3.10/dist-packages (from tensorflow-model-analysis<0.48.0,>=0.47.0->tfx) (7.7.1)\n",
            "Requirement already satisfied: pillow>=9.4.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-model-analysis<0.48.0,>=0.47.0->tfx) (11.0.0)\n",
            "Collecting rouge-score<2,>=0.1.2 (from tensorflow-model-analysis<0.48.0,>=0.47.0->tfx)\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting sacrebleu<4,>=2.3 (from tensorflow-model-analysis<0.48.0,>=0.47.0->tfx)\n",
            "  Downloading sacrebleu-2.4.3-py3-none-any.whl.metadata (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-estimator>=2.10 (from tensorflow-model-analysis<0.48.0,>=0.47.0->tfx)\n",
            "  Downloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: tf-keras>=2 in /usr/local/lib/python3.10/dist-packages (from tensorflow-transform<1.17.0,>=1.16.0->tfx) (2.17.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.17,>=2.16.0->tfx) (0.45.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform<2,>=1.6.2->tfx) (1.62.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.18.0->apache-beam[gcp]<3,>=2.47->tfx) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.18.0->apache-beam[gcp]<3,>=2.47->tfx) (4.9)\n",
            "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigtable<3,>=2.19.0->apache-beam[gcp]<3,>=2.47->tfx) (0.13.1)\n",
            "Requirement already satisfied: opentelemetry-api>=1.27.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-pubsub<3,>=2.1.0->apache-beam[gcp]<3,>=2.47->tfx) (1.29.0)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.27.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-pubsub<3,>=2.1.0->apache-beam[gcp]<3,>=2.47->tfx) (1.29.0)\n",
            "Collecting overrides<8.0.0,>=6.0.1 (from google-cloud-pubsublite<2,>=1.2.0->apache-beam[gcp]<3,>=2.47->tfx)\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: sqlparse>=0.4.4 in /usr/local/lib/python3.10/dist-packages (from google-cloud-spanner<4,>=3.0.0->apache-beam[gcp]<3,>=2.47->tfx) (0.5.3)\n",
            "Collecting grpc-interceptor>=0.15.4 (from google-cloud-spanner<4,>=3.0.0->apache-beam[gcp]<3,>=2.47->tfx)\n",
            "  Downloading grpc_interceptor-0.15.4-py3-none-any.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage<3,>=2.18.2->apache-beam[gcp]<3,>=2.47->tfx) (1.6.0)\n",
            "Collecting docopt (from hdfs<3.0.0,>=2.1.0->apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tfx)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<0.23.0,>=0.8->apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tfx) (3.2.0)\n",
            "Collecting jedi>=0.16 (from ipython<8,>=7->tensorflow-model-analysis<0.48.0,>=0.47.0->tfx)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython<8,>=7->tensorflow-model-analysis<0.48.0,>=0.47.0->tfx) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython<8,>=7->tensorflow-model-analysis<0.48.0,>=0.47.0->tfx) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython<8,>=7->tensorflow-model-analysis<0.48.0,>=0.47.0->tfx) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython<8,>=7->tensorflow-model-analysis<0.48.0,>=0.47.0->tfx) (3.0.48)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython<8,>=7->tensorflow-model-analysis<0.48.0,>=0.47.0->tfx) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython<8,>=7->tensorflow-model-analysis<0.48.0,>=0.47.0->tfx) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython<8,>=7->tensorflow-model-analysis<0.48.0,>=0.47.0->tfx) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython<8,>=7->tensorflow-model-analysis<0.48.0,>=0.47.0->tfx) (4.9.0)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.48.0,>=0.47.0->tfx) (5.5.6)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.48.0,>=0.47.0->tfx) (0.2.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.48.0,>=0.47.0->tfx) (3.6.10)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.48.0,>=0.47.0->tfx) (3.0.13)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.0.0->apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tfx) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.0.0->apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tfx) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.0.0->apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tfx) (0.22.3)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner!=1.4.0,!=1.4.1,<2,>=1.0.4->tfx) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner!=1.4.0,!=1.4.1,<2,>=1.0.4->tfx) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner!=1.4.0,!=1.4.1,<2,>=1.0.4->tfx) (0.13.1)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.10/dist-packages (from oauth2client>=1.4.12->google-apitools<1,>=0.5->tfx) (0.6.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3->google-cloud-aiplatform<2,>=1.6.2->tfx) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3->google-cloud-aiplatform<2,>=1.6.2->tfx) (2.27.1)\n",
            "Collecting dnspython<3.0.0,>=1.16.0 (from pymongo<5.0.0,>=3.8.0->apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tfx)\n",
            "  Downloading dnspython-2.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: async-timeout>=4.0.3 in /usr/local/lib/python3.10/dist-packages (from redis<6,>=5.0.0->apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tfx) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tfx) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tfx) (3.10)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge-score<2,>=0.1.2->tensorflow-model-analysis<0.48.0,>=0.47.0->tfx) (3.9.1)\n",
            "Collecting portalocker (from sacrebleu<4,>=2.3->tensorflow-model-analysis<0.48.0,>=0.47.0->tfx)\n",
            "  Downloading portalocker-3.0.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu<4,>=2.3->tensorflow-model-analysis<0.48.0,>=0.47.0->tfx) (0.9.0)\n",
            "Collecting colorama (from sacrebleu<4,>=2.3->tensorflow-model-analysis<0.48.0,>=0.47.0->tfx)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu<4,>=2.3->tensorflow-model-analysis<0.48.0,>=0.47.0->tfx) (5.3.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16.0->tfx) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16.0->tfx) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16.0->tfx) (3.1.3)\n",
            "Collecting protobuf<5,>=3.20.3 (from tfx)\n",
            "  Downloading protobuf-3.20.3-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (679 bytes)\n",
            "INFO: pip is looking at multiple versions of tf-keras to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting tf-keras>=2 (from tensorflow-transform<1.17.0,>=1.16.0->tfx)\n",
            "  Downloading tf_keras-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "  Downloading tf_keras-2.16.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: keyring in /usr/lib/python3/dist-packages (from keyrings.google-artifactregistry-auth->apache-beam[gcp]<3,>=2.47->tfx) (23.5.0)\n",
            "Requirement already satisfied: pluggy in /usr/local/lib/python3.10/dist-packages (from keyrings.google-artifactregistry-auth->apache-beam[gcp]<3,>=2.47->tfx) (1.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib->kubernetes<27,>=10.0.1->tfx) (3.2.2)\n",
            "INFO: pip is looking at multiple versions of grpcio-status to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting grpcio-status<2.0.dev0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform<2,>=1.6.2->tfx)\n",
            "  Downloading grpcio_status-1.68.1-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.68.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.67.1-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.67.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.66.2-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.66.1-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.66.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "INFO: pip is still looking at multiple versions of grpcio-status to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading grpcio_status-1.65.5-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.65.4-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.65.2-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.65.1-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.64.3-py3-none-any.whl.metadata (1.1 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading grpcio_status-1.64.1-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.64.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.63.2-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.63.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.62.2-py3-none-any.whl.metadata (1.3 kB)\n",
            "  Downloading grpcio_status-1.62.1-py3-none-any.whl.metadata (1.3 kB)\n",
            "  Downloading grpcio_status-1.62.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "  Downloading grpcio_status-1.61.3-py3-none-any.whl.metadata (1.3 kB)\n",
            "  Downloading grpcio_status-1.60.2-py3-none-any.whl.metadata (1.3 kB)\n",
            "  Downloading grpcio_status-1.60.1-py3-none-any.whl.metadata (1.3 kB)\n",
            "  Downloading grpcio_status-1.60.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "  Downloading grpcio_status-1.59.5-py3-none-any.whl.metadata (1.3 kB)\n",
            "  Downloading grpcio_status-1.59.3-py3-none-any.whl.metadata (1.3 kB)\n",
            "  Downloading grpcio_status-1.59.2-py3-none-any.whl.metadata (1.3 kB)\n",
            "  Downloading grpcio_status-1.59.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "  Downloading grpcio_status-1.58.3-py3-none-any.whl.metadata (1.3 kB)\n",
            "  Downloading grpcio_status-1.58.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "  Downloading grpcio_status-1.57.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "  Downloading grpcio_status-1.56.2-py3-none-any.whl.metadata (1.3 kB)\n",
            "  Downloading grpcio_status-1.56.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "  Downloading grpcio_status-1.55.3-py3-none-any.whl.metadata (1.3 kB)\n",
            "  Downloading grpcio_status-1.54.3-py3-none-any.whl.metadata (1.3 kB)\n",
            "  Downloading grpcio_status-1.54.2-py3-none-any.whl.metadata (1.3 kB)\n",
            "  Downloading grpcio_status-1.54.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "  Downloading grpcio_status-1.53.2-py3-none-any.whl.metadata (1.3 kB)\n",
            "  Downloading grpcio_status-1.53.1-py3-none-any.whl.metadata (1.3 kB)\n",
            "  Downloading grpcio_status-1.53.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "  Downloading grpcio_status-1.51.3-py3-none-any.whl.metadata (1.3 kB)\n",
            "  Downloading grpcio_status-1.51.1-py3-none-any.whl.metadata (1.3 kB)\n",
            "  Downloading grpcio_status-1.50.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "  Downloading grpcio_status-1.49.1-py3-none-any.whl.metadata (1.3 kB)\n",
            "  Downloading grpcio_status-1.48.2-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets<8,>=7->tensorflow-model-analysis<0.48.0,>=0.47.0->tfx) (6.1.12)\n",
            "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets<8,>=7->tensorflow-model-analysis<0.48.0,>=0.47.0->tfx) (6.3.3)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython<8,>=7->tensorflow-model-analysis<0.48.0,>=0.47.0->tfx) (0.8.4)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.27.0->google-cloud-pubsub<3,>=2.1.0->apache-beam[gcp]<3,>=2.47->tfx) (1.2.15)\n",
            "Requirement already satisfied: importlib-metadata<=8.5.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.27.0->google-cloud-pubsub<3,>=2.1.0->apache-beam[gcp]<3,>=2.47->tfx) (8.5.0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.50b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-sdk>=1.27.0->google-cloud-pubsub<3,>=2.1.0->apache-beam[gcp]<3,>=2.47->tfx) (0.50b0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython<8,>=7->tensorflow-model-analysis<0.48.0,>=0.47.0->tfx) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython<8,>=7->tensorflow-model-analysis<0.48.0,>=0.47.0->tfx) (0.2.13)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.10/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.48.0,>=0.47.0->tfx) (6.5.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score<2,>=0.1.2->tensorflow-model-analysis<0.48.0,>=0.47.0->tfx) (4.67.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras->keras-tuner!=1.4.0,!=1.4.1,<2,>=1.0.4->tfx) (3.0.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<=8.5.0,>=6.0->opentelemetry-api>=1.27.0->google-cloud-pubsub<3,>=2.1.0->apache-beam[gcp]<3,>=2.47->tfx) (3.21.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras->keras-tuner!=1.4.0,!=1.4.1,<2,>=1.0.4->tfx) (0.1.2)\n",
            "Requirement already satisfied: pyzmq<25,>=17 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.48.0,>=0.47.0->tfx) (24.0.1)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.48.0,>=0.47.0->tfx) (23.1.0)\n",
            "Requirement already satisfied: jupyter-core>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.48.0,>=0.47.0->tfx) (5.7.2)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.48.0,>=0.47.0->tfx) (5.10.4)\n",
            "Requirement already satisfied: nbconvert>=5 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.48.0,>=0.47.0->tfx) (7.16.4)\n",
            "Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.48.0,>=0.47.0->tfx) (1.6.0)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.48.0,>=0.47.0->tfx) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.48.0,>=0.47.0->tfx) (0.18.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.48.0,>=0.47.0->tfx) (0.21.1)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.48.0,>=0.47.0->tfx) (1.1.0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core>=4.6.1->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.48.0,>=0.47.0->tfx) (4.3.6)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.48.0,>=0.47.0->tfx) (0.2.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.48.0,>=0.47.0->tfx) (4.12.3)\n",
            "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.48.0,>=0.47.0->tfx) (6.2.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.48.0,>=0.47.0->tfx) (0.7.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.48.0,>=0.47.0->tfx) (0.3.0)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.48.0,>=0.47.0->tfx) (3.0.2)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.48.0,>=0.47.0->tfx) (0.10.1)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.48.0,>=0.47.0->tfx) (1.5.1)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.48.0,>=0.47.0->tfx) (1.4.0)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.10/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.48.0,>=0.47.0->tfx) (2.21.1)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.48.0,>=0.47.0->tfx) (21.2.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.48.0,>=0.47.0->tfx) (0.5.1)\n",
            "Requirement already satisfied: jupyter-server<3,>=1.8 in /usr/local/lib/python3.10/dist-packages (from notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.48.0,>=0.47.0->tfx) (1.24.0)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.48.0,>=0.47.0->tfx) (1.17.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.48.0,>=0.47.0->tfx) (2.6)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.48.0,>=0.47.0->tfx) (2.22)\n",
            "Requirement already satisfied: anyio<4,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.48.0,>=0.47.0->tfx) (3.7.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.48.0,>=0.47.0->tfx) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.48.0,>=0.47.0->tfx) (1.2.2)\n",
            "Downloading tfx-1.16.0-py3-none-any.whl (7.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m71.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ml_pipelines_sdk-1.16.0-py3-none-any.whl (7.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scikit_learn-1.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.4/13.4 MB\u001b[0m \u001b[31m82.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading apache_beam-2.61.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.8/15.8 MB\u001b[0m \u001b[31m76.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading attrs-23.2.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docker-7.1.0-py3-none-any.whl (147 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_api_python_client-1.12.11-py2.py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading grpcio-1.65.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m91.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading keras_tuner-1.4.7-py3-none-any.whl (129 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kubernetes-26.1.0-py2.py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m59.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ml_metadata-1.16.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m89.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow-10.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.9/35.9 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.4/38.4 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow-2.16.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (590.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m590.6/590.6 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_data_validation-1.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.0/19.0 MB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_hub-0.15.0-py2.py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_model_analysis-0.47.1-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_serving_api-2.16.1-py2.py3-none-any.whl (26 kB)\n",
            "Downloading tensorflow_transform-1.16.0-py3-none-any.whl (451 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m451.5/451.5 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tfx_bsl-1.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (22.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.5/22.5 MB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cloudpickle-2.2.1-py3-none-any.whl (25 kB)\n",
            "Downloading fastavro-1.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m45.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fasteners-0.19-py3-none-any.whl (18 kB)\n",
            "Downloading google_cloud_dlp-3.26.0-py2.py3-none-any.whl (204 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.2/204.2 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_cloud_pubsublite-1.11.1-py2.py3-none-any.whl (304 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m304.6/304.6 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_cloud_recommendations_ai-0.10.15-py2.py3-none-any.whl (206 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m206.5/206.5 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_cloud_spanner-3.51.0-py2.py3-none-any.whl (432 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m432.6/432.6 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_cloud_videointelligence-2.15.0-py2.py3-none-any.whl (269 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m269.8/269.8 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_cloud_vision-3.9.0-py2.py3-none-any.whl (514 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m514.6/514.6 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpickle-3.4.2-py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.3/46.3 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ml_dtypes-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading objsize-0.7.0-py3-none-any.whl (11 kB)\n",
            "Downloading pandas-1.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
            "Downloading pydot-1.4.2-py2.py3-none-any.whl (21 kB)\n",
            "Downloading pymongo-4.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading redis-5.2.1-py3-none-any.whl (261 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.5/261.5 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sacrebleu-2.4.3-py3-none-any.whl (103 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.0/104.0 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
            "Downloading tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m43.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.0/442.0 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_metadata-1.16.1-py3-none-any.whl (28 kB)\n",
            "Downloading protobuf-3.20.3-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tf_keras-2.16.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uritemplate-3.0.1-py2.py3-none-any.whl (15 kB)\n",
            "Downloading zstandard-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading keyrings.google_artifactregistry_auth-1.1.2-py3-none-any.whl (10 kB)\n",
            "Downloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n",
            "Downloading dnspython-2.7.0-py3-none-any.whl (313 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.6/313.6 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading grpc_interceptor-0.15.4-py3-none-any.whl (20 kB)\n",
            "Downloading grpcio_status-1.48.2-py3-none-any.whl (14 kB)\n",
            "Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading portalocker-3.0.0-py3-none-any.whl (19 kB)\n",
            "Building wheels for collected packages: google-apitools, crcmod, dill, hdfs, pyfarmhash, rouge-score, docopt\n",
            "  Building wheel for google-apitools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for google-apitools: filename=google_apitools-0.5.31-py3-none-any.whl size=131014 sha256=84c7d1c3fcc205060c9970640dffbc88a431c24374e8e6339557ae1d23c35250\n",
            "  Stored in directory: /root/.cache/pip/wheels/04/b7/e0/9712f8c23a5da3d9d16fb88216b897bf60e85b12f5470f26ee\n",
            "  Building wheel for crcmod (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for crcmod: filename=crcmod-1.7-cp310-cp310-linux_x86_64.whl size=31410 sha256=698b7a93af44ff3a873b4a5c98d9312da085af6e6b80544b4fbf7f60399e91e4\n",
            "  Stored in directory: /root/.cache/pip/wheels/85/4c/07/72215c529bd59d67e3dac29711d7aba1b692f543c808ba9e86\n",
            "  Building wheel for dill (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dill: filename=dill-0.3.1.1-py3-none-any.whl size=78542 sha256=3b26354d88c18617fed1a7e16db314be8f9095c628e21c8bb451ffe4b24c3abb\n",
            "  Stored in directory: /root/.cache/pip/wheels/ea/e2/86/64980d90e297e7bf2ce588c2b96e818f5399c515c4bb8a7e4f\n",
            "  Building wheel for hdfs (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hdfs: filename=hdfs-2.7.3-py3-none-any.whl size=34324 sha256=397ff9948c951d2d31acdfd2bdd2b991704e1acf04cc2ac684438e81512c7cbd\n",
            "  Stored in directory: /root/.cache/pip/wheels/e5/8d/b6/99c1c0a3ac5788c866b0ecd3f48b0134a5910e6ed26011800b\n",
            "  Building wheel for pyfarmhash (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyfarmhash: filename=pyfarmhash-0.3.2-cp310-cp310-linux_x86_64.whl size=88656 sha256=4a3ff5c18589d716a5117ad662fea13eed7745d24b6318e24719b3c711d7a9da\n",
            "  Stored in directory: /root/.cache/pip/wheels/e0/08/da/f66b1f3258fe3f1e767b2136c5444dbfa9fa3f7944cc5e1983\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=cfeb1a43613673a3d8b6beed05ad5a0ba7a242b83e1e3f454529d795b8c23f1f\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=06a9e06aeec054efb911ede2f1c443bfc03102663fa6f8671fff75e4ed1d4516\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "Successfully built google-apitools crcmod dill hdfs pyfarmhash rouge-score docopt\n",
            "Installing collected packages: sortedcontainers, pyfarmhash, kt-legacy, docopt, crcmod, zstandard, uritemplate, tensorflow-estimator, scipy, redis, pydot, pyarrow-hotfix, pyarrow, protobuf, portalocker, overrides, objsize, ml-dtypes, jsonpickle, jedi, grpcio, fasteners, fastavro, dnspython, dill, colorama, cloudpickle, attrs, tensorflow-metadata, tensorflow-hub, tensorboard, scikit-learn, sacrebleu, rouge-score, pymongo, pandas, ml-metadata, hdfs, grpc-interceptor, docker, kubernetes, keyrings.google-artifactregistry-auth, grpcio-status, google-apitools, tensorflow, keras-tuner, google-api-python-client, tf-keras, tensorflow-serving-api, ml-pipelines-sdk, google-cloud-vision, google-cloud-videointelligence, google-cloud-spanner, google-cloud-recommendations-ai, google-cloud-dlp, apache-beam, google-cloud-pubsublite, tfx-bsl, tensorflow-transform, tensorflow-data-validation, tensorflow-model-analysis, tfx\n",
            "  Attempting uninstall: uritemplate\n",
            "    Found existing installation: uritemplate 4.1.1\n",
            "    Uninstalling uritemplate-4.1.1:\n",
            "      Successfully uninstalled uritemplate-4.1.1\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.13.1\n",
            "    Uninstalling scipy-1.13.1:\n",
            "      Successfully uninstalled scipy-1.13.1\n",
            "  Attempting uninstall: pydot\n",
            "    Found existing installation: pydot 3.0.3\n",
            "    Uninstalling pydot-3.0.3:\n",
            "      Successfully uninstalled pydot-3.0.3\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 17.0.0\n",
            "    Uninstalling pyarrow-17.0.0:\n",
            "      Successfully uninstalled pyarrow-17.0.0\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 4.25.5\n",
            "    Uninstalling protobuf-4.25.5:\n",
            "      Successfully uninstalled protobuf-4.25.5\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml-dtypes 0.4.1\n",
            "    Uninstalling ml-dtypes-0.4.1:\n",
            "      Successfully uninstalled ml-dtypes-0.4.1\n",
            "  Attempting uninstall: jsonpickle\n",
            "    Found existing installation: jsonpickle 4.0.1\n",
            "    Uninstalling jsonpickle-4.0.1:\n",
            "      Successfully uninstalled jsonpickle-4.0.1\n",
            "  Attempting uninstall: grpcio\n",
            "    Found existing installation: grpcio 1.68.1\n",
            "    Uninstalling grpcio-1.68.1:\n",
            "      Successfully uninstalled grpcio-1.68.1\n",
            "  Attempting uninstall: cloudpickle\n",
            "    Found existing installation: cloudpickle 3.1.0\n",
            "    Uninstalling cloudpickle-3.1.0:\n",
            "      Successfully uninstalled cloudpickle-3.1.0\n",
            "  Attempting uninstall: attrs\n",
            "    Found existing installation: attrs 24.3.0\n",
            "    Uninstalling attrs-24.3.0:\n",
            "      Successfully uninstalled attrs-24.3.0\n",
            "  Attempting uninstall: tensorflow-metadata\n",
            "    Found existing installation: tensorflow-metadata 1.13.1\n",
            "    Uninstalling tensorflow-metadata-1.13.1:\n",
            "      Successfully uninstalled tensorflow-metadata-1.13.1\n",
            "  Attempting uninstall: tensorflow-hub\n",
            "    Found existing installation: tensorflow-hub 0.16.1\n",
            "    Uninstalling tensorflow-hub-0.16.1:\n",
            "      Successfully uninstalled tensorflow-hub-0.16.1\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.17.1\n",
            "    Uninstalling tensorboard-2.17.1:\n",
            "      Successfully uninstalled tensorboard-2.17.1\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.6.0\n",
            "    Uninstalling scikit-learn-1.6.0:\n",
            "      Successfully uninstalled scikit-learn-1.6.0\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "  Attempting uninstall: grpcio-status\n",
            "    Found existing installation: grpcio-status 1.62.3\n",
            "    Uninstalling grpcio-status-1.62.3:\n",
            "      Successfully uninstalled grpcio-status-1.62.3\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.17.1\n",
            "    Uninstalling tensorflow-2.17.1:\n",
            "      Successfully uninstalled tensorflow-2.17.1\n",
            "  Attempting uninstall: google-api-python-client\n",
            "    Found existing installation: google-api-python-client 2.155.0\n",
            "    Uninstalling google-api-python-client-2.155.0:\n",
            "      Successfully uninstalled google-api-python-client-2.155.0\n",
            "  Attempting uninstall: tf-keras\n",
            "    Found existing installation: tf_keras 2.17.0\n",
            "    Uninstalling tf_keras-2.17.0:\n",
            "      Successfully uninstalled tf_keras-2.17.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.10.1 requires pandas<2.2.3dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "cudf-cu12 24.10.1 requires pyarrow<18.0.0a0,>=14.0.0, but you have pyarrow 10.0.1 which is incompatible.\n",
            "dask 2024.10.0 requires cloudpickle>=3.0.0, but you have cloudpickle 2.2.1 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 1.5.3 which is incompatible.\n",
            "mizani 0.13.1 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "plotnine 0.14.4 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "pylibcudf-cu12 24.10.1 requires pyarrow<18.0.0a0,>=14.0.0, but you have pyarrow 10.0.1 which is incompatible.\n",
            "xarray 2024.11.0 requires pandas>=2.1, but you have pandas 1.5.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed apache-beam-2.61.0 attrs-23.2.0 cloudpickle-2.2.1 colorama-0.4.6 crcmod-1.7 dill-0.3.1.1 dnspython-2.7.0 docker-7.1.0 docopt-0.6.2 fastavro-1.10.0 fasteners-0.19 google-api-python-client-1.12.11 google-apitools-0.5.31 google-cloud-dlp-3.26.0 google-cloud-pubsublite-1.11.1 google-cloud-recommendations-ai-0.10.15 google-cloud-spanner-3.51.0 google-cloud-videointelligence-2.15.0 google-cloud-vision-3.9.0 grpc-interceptor-0.15.4 grpcio-1.65.5 grpcio-status-1.48.2 hdfs-2.7.3 jedi-0.19.2 jsonpickle-3.4.2 keras-tuner-1.4.7 keyrings.google-artifactregistry-auth-1.1.2 kt-legacy-1.0.5 kubernetes-26.1.0 ml-dtypes-0.3.2 ml-metadata-1.16.0 ml-pipelines-sdk-1.16.0 objsize-0.7.0 overrides-7.7.0 pandas-1.5.3 portalocker-3.0.0 protobuf-3.20.3 pyarrow-10.0.1 pyarrow-hotfix-0.6 pydot-1.4.2 pyfarmhash-0.3.2 pymongo-4.10.1 redis-5.2.1 rouge-score-0.1.2 sacrebleu-2.4.3 scikit-learn-1.5.1 scipy-1.12.0 sortedcontainers-2.4.0 tensorboard-2.16.2 tensorflow-2.16.2 tensorflow-data-validation-1.16.1 tensorflow-estimator-2.15.0 tensorflow-hub-0.15.0 tensorflow-metadata-1.16.1 tensorflow-model-analysis-0.47.1 tensorflow-serving-api-2.16.1 tensorflow-transform-1.16.0 tf-keras-2.16.0 tfx-1.16.0 tfx-bsl-1.16.1 uritemplate-3.0.1 zstandard-0.23.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "1ea795698b2a491ca61e2b01a4099a8b"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "pip install tfx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cKfhxdkKuTEz",
        "outputId": "3e05b4d9-d902-4a90-9414-98589f78c772"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "pip show tfx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ip0jzKg9GVI"
      },
      "outputs": [],
      "source": [
        "pip show tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8QPEkzF0mVN"
      },
      "outputs": [],
      "source": [
        "pip install tensorflow --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow-model-analysis==0.46.0"
      ],
      "metadata": {
        "id": "5yC22aBVc3VJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow==2.15"
      ],
      "metadata": {
        "id": "mLwqs3anc33o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWie4tOujO1n"
      },
      "source": [
        "# Import Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "f-N0Q50AXmI-"
      },
      "outputs": [],
      "source": [
        "# Import library\n",
        "import os\n",
        "from typing import Text\n",
        "from absl import logging\n",
        "from tfx.orchestration import metadata, pipeline\n",
        "from tfx.orchestration.beam.beam_dag_runner import BeamDagRunner"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jPYe9R3jdPK"
      },
      "source": [
        "Melakukan set variabel seperti pipeline name, path untuk menyimpan output, path module, dan banyak lainnya."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "0XE3Q2omXot_"
      },
      "outputs": [],
      "source": [
        "# Nama pipeline\n",
        "PIPELINE_NAME = \"heart-disease-pipeline\"\n",
        "\n",
        "# Pipeline inputs\n",
        "DATA_ROOT = \"/content/data\"\n",
        "TRANSFORM_MODULE_FILE = \"/content/modules/transform.py\"\n",
        "TUNER_MODULE_FILE = \"/content/modules/tuner.py\"\n",
        "TRAINER_MODULE_FILE = \"/content/modules/trainer.py\"\n",
        "COMPONENTS_MODULE_FILE = \"/content/modules/components.py\"\n",
        "\n",
        "# Pipeline outputs\n",
        "OUTPUT_BASE = \"output\"\n",
        "serving_model_dir = os.path.join(OUTPUT_BASE, 'serving_model')\n",
        "pipeline_root = os.path.join(OUTPUT_BASE, PIPELINE_NAME)\n",
        "metadata_path = os.path.join(pipeline_root, \"metadata.sqlite\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rz63VCfhjlXn"
      },
      "source": [
        "Pembuatan pipeline component module file menggunakan magic command. Pipeline terdiri dari:\n",
        "\n",
        "1. CsvExampleGen\n",
        "2. StatisticsGen\n",
        "3. SchemaGen\n",
        "4. ExampleValidator\n",
        "5. Transform\n",
        "6. Trainer\n",
        "7. Evaluator\n",
        "8. Pusher\n",
        "\n",
        "Komponen trainer sudah menggunakan komponen tuner. Pusher akan melakukan push model jika melebihi syarat dari BinaryAccuracy 0.5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bg17vcTIkUk-"
      },
      "source": [
        "# Membuat Pipeline Components"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ntORvxlBXorj",
        "outputId": "e5f42823-e3e9-48f1-8d86-3953ee661fae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/modules/components.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile {COMPONENTS_MODULE_FILE}\n",
        "\n",
        "# Import library\n",
        "import os\n",
        "import tensorflow_model_analysis as tfma\n",
        "from tfx.components import (\n",
        "    CsvExampleGen,\n",
        "    StatisticsGen,\n",
        "    SchemaGen,\n",
        "    ExampleValidator,\n",
        "    Transform,\n",
        "    Tuner,\n",
        "    Trainer,\n",
        "    Evaluator,\n",
        "    Pusher\n",
        ")\n",
        "from tfx.proto import example_gen_pb2, trainer_pb2, pusher_pb2\n",
        "from tfx.types import Channel\n",
        "from tfx.dsl.components.common.resolver import Resolver\n",
        "from tfx.types.standard_artifacts import Model, ModelBlessing\n",
        "from tfx.dsl.input_resolution.strategies.latest_blessed_model_strategy import (\n",
        "    LatestBlessedModelStrategy)\n",
        "\n",
        "# Fungsi untuk melakukan inisialisasi components\n",
        "def init_components(config):\n",
        "\n",
        "    \"\"\"Returns tfx components for the pipeline.\n",
        "\n",
        "    Args:\n",
        "        data_dir (str): Directory containing the dataset.\n",
        "        transform_module (str): Path to the transform module.\n",
        "        tuner_module (str): Path to the tuner module.\n",
        "        training_module (str): Path to the training module.\n",
        "        training_steps (int): Number of training steps.\n",
        "        eval_steps (int): Number of evaluation steps.\n",
        "        serving_model_dir (str): Directory to save the serving\n",
        "\n",
        "    Returns:\n",
        "        components: Tuple of TFX components.\n",
        "    \"\"\"\n",
        "\n",
        "    # Membagi dataset dengan perbandingan 8:2\n",
        "    output = example_gen_pb2.Output(\n",
        "        split_config = example_gen_pb2.SplitConfig(splits=[\n",
        "            example_gen_pb2.SplitConfig.Split(name=\"train\", hash_buckets=8),\n",
        "            example_gen_pb2.SplitConfig.Split(name=\"eval\", hash_buckets=2)\n",
        "        ])\n",
        "    )\n",
        "\n",
        "    # Komponen example gen\n",
        "    example_gen = CsvExampleGen(\n",
        "        input_base=config[\"DATA_ROOT\"],\n",
        "        output_config=output\n",
        "    )\n",
        "\n",
        "    # Komponen statistics gen\n",
        "    statistics_gen = StatisticsGen(\n",
        "        examples=example_gen.outputs[\"examples\"]\n",
        "    )\n",
        "\n",
        "    # Komponen schema gen\n",
        "    schema_gen = SchemaGen(\n",
        "        statistics=statistics_gen.outputs[\"statistics\"]\n",
        "    )\n",
        "\n",
        "    # Komponen example validator\n",
        "    example_validator = ExampleValidator(\n",
        "        statistics=statistics_gen.outputs['statistics'],\n",
        "        schema=schema_gen.outputs['schema']\n",
        "    )\n",
        "\n",
        "    # Komponen transform. Menggunakan module transform.py\n",
        "    transform  = Transform(\n",
        "        examples=example_gen.outputs['examples'],\n",
        "        schema= schema_gen.outputs['schema'],\n",
        "        module_file=os.path.abspath(config[\"transform_module\"])\n",
        "    )\n",
        "\n",
        "    # Komponen tuner. Menggunakan module tuner.py\n",
        "    tuner = Tuner(\n",
        "        module_file=os.path.abspath(config[\"tuner_module\"]),\n",
        "        examples=transform.outputs['transformed_examples'],\n",
        "        transform_graph=transform.outputs['transform_graph'],\n",
        "        schema=schema_gen.outputs['schema'],\n",
        "        train_args=trainer_pb2.TrainArgs(\n",
        "            splits=['train'],\n",
        "            num_steps=config[\"training_steps\"]),\n",
        "        eval_args=trainer_pb2.EvalArgs(\n",
        "            splits=['eval'],\n",
        "            num_steps=config[\"eval_steps\"]),\n",
        "    )\n",
        "\n",
        "    # Komponen trainer. Menggunakan module trainer.py\n",
        "    trainer  = Trainer(\n",
        "        module_file=os.path.abspath(config[\"training_module\"]),\n",
        "        examples = transform.outputs['transformed_examples'],\n",
        "        transform_graph=transform.outputs['transform_graph'],\n",
        "        schema=schema_gen.outputs['schema'],\n",
        "        hyperparameters=tuner.outputs['best_hyperparameters'],\n",
        "        train_args=trainer_pb2.TrainArgs(\n",
        "            splits=['train'],\n",
        "            num_steps=config[\"training_steps\"]),\n",
        "        eval_args=trainer_pb2.EvalArgs(\n",
        "            splits=['eval'],\n",
        "            num_steps=config[\"eval_steps\"])\n",
        "    )\n",
        "\n",
        "    # Komponen model resolver\n",
        "    model_resolver = Resolver(\n",
        "        strategy_class= LatestBlessedModelStrategy,\n",
        "        model = Channel(type=Model),\n",
        "        model_blessing = Channel(type=ModelBlessing)\n",
        "    ).with_id('Latest_blessed_model_resolver')\n",
        "\n",
        "    metrics_specs = [\n",
        "        tfma.MetricsSpec(metrics=[\n",
        "                tfma.MetricConfig(class_name='AUC'),\n",
        "                tfma.MetricConfig(class_name=\"Precision\"),\n",
        "                tfma.MetricConfig(class_name=\"Recall\"),\n",
        "                tfma.MetricConfig(class_name=\"ExampleCount\"),\n",
        "                tfma.MetricConfig(class_name='BinaryAccuracy',\n",
        "                    threshold=tfma.MetricThreshold(\n",
        "                        value_threshold=tfma.GenericValueThreshold(\n",
        "                            lower_bound={'value':0.8}),\n",
        "                        change_threshold=tfma.GenericChangeThreshold(\n",
        "                            direction=tfma.MetricDirection.HIGHER_IS_BETTER,\n",
        "                            absolute={'value':0.0001})\n",
        "                        )\n",
        "                )\n",
        "            ])\n",
        "    ]\n",
        "\n",
        "\n",
        "    eval_config = tfma.EvalConfig(\n",
        "    model_specs=[tfma.ModelSpec(label_key='target')],  # Ensure 'target' is the correct label\n",
        "    slicing_specs=[tfma.SlicingSpec()],\n",
        "    metrics_specs=metrics_specs\n",
        "    )\n",
        "\n",
        "\n",
        "    # Komponen evaluator\n",
        "    evaluator = Evaluator(\n",
        "        examples=example_gen.outputs['examples'],\n",
        "        model=trainer.outputs['model'],\n",
        "        baseline_model=model_resolver.outputs['model'],\n",
        "        eval_config=eval_config)\n",
        "\n",
        "    # Komponen pusher\n",
        "    pusher = Pusher(\n",
        "        model=trainer.outputs[\"model\"],\n",
        "        model_blessing=evaluator.outputs[\"blessing\"],\n",
        "        push_destination=pusher_pb2.PushDestination(\n",
        "            filesystem=pusher_pb2.PushDestination.Filesystem(\n",
        "                base_directory=config[\"serving_model_dir\"]\n",
        "            )\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    # Mengembalikan semua komponen\n",
        "    components = (\n",
        "        example_gen,\n",
        "        statistics_gen,\n",
        "        schema_gen,\n",
        "        example_validator,\n",
        "        transform,\n",
        "        tuner,\n",
        "        trainer,\n",
        "        model_resolver,\n",
        "        evaluator,\n",
        "        pusher\n",
        "    )\n",
        "\n",
        "    # Mengembalikan komponen\n",
        "    return components"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCrACnNjka9B"
      },
      "source": [
        "# Data Transform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HcfFLEq5XopQ",
        "outputId": "5dbd3cd3-9622-4df0-d85e-8fa5aaab0b15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/modules/transform.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile {TRANSFORM_MODULE_FILE}\n",
        "\n",
        "# Import library\n",
        "import tensorflow as tf\n",
        "import tensorflow_transform as tft\n",
        "\n",
        "LABEL_KEY = \"target\"\n",
        "\n",
        "# List of feature names and types\n",
        "NUMERIC_FEATURES = ['age', 'ca', 'chol', 'oldpeak', 'thalach', 'trestbps']\n",
        "CATEGORICAL_FEATURES = ['cp', 'exang', 'fbs', 'restecg', 'sex', 'slope', 'thal']\n",
        "\n",
        "def transformed_name(key):\n",
        "    \"\"\"Renaming transformed features\"\"\"\n",
        "    return key + \"_xf\"\n",
        "\n",
        "def preprocessing_fn(inputs):\n",
        "    \"\"\"\n",
        "    Preprocess input features into transformed features.\n",
        "\n",
        "    Args:\n",
        "        inputs: dictionary of raw input features.\n",
        "\n",
        "    Returns:\n",
        "        outputs: dictionary of transformed features.\n",
        "    \"\"\"\n",
        "    outputs = {}\n",
        "\n",
        "    # Filter out rows with invalid values for 'ca' and 'thal'\n",
        "    valid_rows = tf.logical_and(\n",
        "        tf.not_equal(inputs['ca'], 4),  # Exclude rows where `ca` is 4\n",
        "        tf.not_equal(inputs['thal'], 0)  # Exclude rows where `thal` is 0\n",
        "    )\n",
        "\n",
        "    # Apply the valid row mask to all inputs\n",
        "    filtered_inputs = {key: tf.boolean_mask(inputs[key], valid_rows) for key in inputs}\n",
        "\n",
        "    # Normalize numeric features\n",
        "    for feature in NUMERIC_FEATURES:\n",
        "        outputs[transformed_name(feature)] = tft.scale_to_z_score(filtered_inputs[feature])\n",
        "\n",
        "    # Label encode categorical features\n",
        "    for feature in CATEGORICAL_FEATURES:\n",
        "        outputs[transformed_name(feature)] = tft.compute_and_apply_vocabulary(filtered_inputs[feature])\n",
        "\n",
        "    # Include the label\n",
        "    outputs[LABEL_KEY] = filtered_inputs[LABEL_KEY]\n",
        "\n",
        "    return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogzyQw1lkec1"
      },
      "source": [
        "Kode di atas adalah sebuah modul pemrosesan data untuk pra-pemrosesan fitur pada dataset menggunakan TensorFlow Transform (TFT). Berikut adalah penjelasan singkat dari proses yang dilakukan:\n",
        "\n",
        "1. Fitur Numerik: Fitur numerik seperti usia, kolesterol, dan tekanan darah diubah agar memiliki distribusi yang lebih baik dengan melakukan normalisasi menggunakan teknik z-score (standarisasi).\n",
        "\n",
        "2. Fitur Kategorikal: Fitur kategorikal seperti jenis kelamin dan hasil tes kesehatan lainnya diberi representasi angka (label encoding) berdasarkan frekuensi kemunculan kategori tersebut dalam dataset.\n",
        "\n",
        "3. Filter Data Tidak Valid: Beberapa baris data yang memiliki nilai tidak valid, seperti nilai ca yang bernilai 4 atau thal yang bernilai 0, dihapus dari dataset.\n",
        "\n",
        "4. Fungsi Utama: Fungsi preprocessing_fn mengatur semua langkah pemrosesan data ini dengan menerima data mentah sebagai input dan mengembalikan data yang telah diproses sesuai dengan transformasi yang telah ditentukan.\n",
        "\n",
        "Proses ini penting untuk membersihkan dan menyiapkan data sebelum digunakan dalam model machine learning."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {TUNER_MODULE_FILE}\n",
        "\n",
        "# Import library\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import kerastuner as kt\n",
        "import tensorflow_transform as tft\n",
        "import keras_tuner as kt\n",
        "from tfx.v1.components import TunerFnResult\n",
        "from tfx.components.trainer.fn_args_utils import FnArgs\n",
        "\n",
        "# Definisikan nama label\n",
        "LABEL_KEY = \"target\"\n",
        "\n",
        "# Daftar fitur numerik dan kategorikal\n",
        "NUMERIC_FEATURES = ['age', 'ca', 'chol', 'oldpeak', 'thalach', 'trestbps']\n",
        "CATEGORICAL_FEATURES = ['cp', 'exang', 'fbs', 'restecg', 'sex', 'slope', 'thal']\n",
        "\n",
        "def transformed_name(key):\n",
        "    \"\"\"Menambahkan suffix '_xf' pada nama fitur yang telah ditransformasi\"\"\"\n",
        "    return key + \"_xf\"\n",
        "\n",
        "# Fungsi untuk membaca data yang telah di-compress\n",
        "def gzip_reader_fn(filenames):\n",
        "    return tf.data.TFRecordDataset(filenames, compression_type='GZIP')\n",
        "\n",
        "# Fungsi untuk membuat model\n",
        "def model_builder(hyperparameters):\n",
        "    \"\"\"\n",
        "    This function defines a Keras model and returns the model as a\n",
        "    Keras object.\n",
        "    \"\"\"\n",
        "\n",
        "    input_features = []\n",
        "\n",
        "    # Menambahkan input layer untuk setiap fitur\n",
        "    for feature in NUMERIC_FEATURES + CATEGORICAL_FEATURES:\n",
        "        input_features.append(tf.keras.Input(shape=(1,), name=transformed_name(feature)))\n",
        "\n",
        "    concatenate = tf.keras.layers.concatenate(input_features)\n",
        "\n",
        "    deep = tf.keras.layers.Dense(hyperparameters.Choice(\n",
        "        'unit_1', [8,16]),\n",
        "        activation=\"relu\")(concatenate)\n",
        "    deep = tf.keras.layers.Dense(hyperparameters.Choice(\n",
        "        'unit_2', [16,32]),\n",
        "        activation=\"relu\")(deep)\n",
        "    deep = tf.keras.layers.Dense(hyperparameters.Choice(\n",
        "        'unit_3', [16,32]),\n",
        "        activation=\"relu\")(deep)\n",
        "    deep = tf.keras.layers.Dense(hyperparameters.Choice(\n",
        "        'unit_4', [32,64]),\n",
        "        activation=\"relu\")(deep)\n",
        "\n",
        "    outputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(deep)\n",
        "\n",
        "    model = tf.keras.models.Model(inputs=input_features, outputs=outputs)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(\n",
        "            learning_rate=hyperparameters.Choice(\n",
        "                'learning_rate', [0.01,0.001])),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=[tf.keras.metrics.BinaryAccuracy()]\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "# Fungsi input untuk mempersiapkan dataset\n",
        "def input_fn(file_pattern, tf_transform_output, batch_size=64):\n",
        "    # Mendapatkan feature_spec untuk fitur yang sudah ditransformasi\n",
        "    transform_feature_spec = tf_transform_output.transformed_feature_spec().copy()\n",
        "\n",
        "    # Membaca data dalam bentuk batch\n",
        "    dataset = tf.data.experimental.make_batched_features_dataset(\n",
        "        file_pattern=file_pattern,\n",
        "        batch_size=batch_size,\n",
        "        features=transform_feature_spec,\n",
        "        reader=gzip_reader_fn,\n",
        "        label_key='target'  # Menggunakan 'target' sebagai label key yang benar\n",
        "    )\n",
        "\n",
        "    # Fungsi untuk format data (menyesuaikan label dan fitur)\n",
        "    def format_data(features, labels):\n",
        "        labels = tf.reshape(labels, [-1, 1])  # Bentuk label sesuai dengan output\n",
        "        return features, labels\n",
        "\n",
        "    return dataset.map(format_data)\n",
        "\n",
        "# Fungsi tuner_fn\n",
        "def tuner_fn(fn_args: FnArgs):\n",
        "    tf_transform_output = tft.TFTransformOutput(fn_args.transform_graph_path)\n",
        "\n",
        "    train_dataset = input_fn(fn_args.train_files, tf_transform_output, batch_size=10)\n",
        "    eval_dataset = input_fn(fn_args.eval_files, tf_transform_output, batch_size=10)\n",
        "\n",
        "    tuner = kt.RandomSearch(\n",
        "        model_builder,\n",
        "        objective=kt.Objective(\"val_binary_accuracy\", direction=\"max\"),\n",
        "        max_trials=10,\n",
        "        directory=os.path.join(fn_args.working_dir, 'tuner'),\n",
        "        project_name='kt_random_search'\n",
        "    )\n",
        "\n",
        "    tuner.search_space_summary()\n",
        "\n",
        "    return TunerFnResult(\n",
        "        tuner=tuner,\n",
        "        fit_kwargs={\n",
        "            \"x\": train_dataset,\n",
        "            'validation_data': eval_dataset,\n",
        "            'steps_per_epoch': fn_args.train_steps,\n",
        "            'validation_steps': fn_args.eval_steps,\n",
        "            \"epochs\": 10\n",
        "        }\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8MwGbdG1GeN8",
        "outputId": "47ab51a7-19ce-4f0d-cb75-cff15abd14f2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/modules/tuner.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ra3n2o9Vkg2K"
      },
      "source": [
        "# Model Development"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "USOxRW5PXonW",
        "outputId": "d381a6bf-3bc1-425e-e4af-90c715af4144"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/modules/trainer.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile {TRAINER_MODULE_FILE}\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_transform as tft\n",
        "from tfx.components.trainer.fn_args_utils import FnArgs\n",
        "from keras.utils import plot_model\n",
        "\n",
        "LABEL_KEY = \"target\"\n",
        "NUMERIC_FEATURES = ['age', 'ca', 'chol', 'oldpeak', 'thalach', 'trestbps']\n",
        "CATEGORICAL_FEATURES = ['cp', 'exang', 'fbs', 'restecg', 'sex', 'slope', 'thal']\n",
        "\n",
        "def transformed_name(key):\n",
        "    return key + \"_xf\"\n",
        "\n",
        "def gzip_reader_fn(filenames):\n",
        "    return tf.data.TFRecordDataset(filenames, compression_type='GZIP')\n",
        "\n",
        "def input_fn(file_pattern, tf_transform_output, batch_size=64):\n",
        "    transform_feature_spec = tf_transform_output.transformed_feature_spec().copy()\n",
        "\n",
        "    dataset = tf.data.experimental.make_batched_features_dataset(\n",
        "        file_pattern=file_pattern,\n",
        "        batch_size=batch_size,\n",
        "        features=transform_feature_spec,\n",
        "        reader=gzip_reader_fn,\n",
        "        label_key=LABEL_KEY\n",
        "    )\n",
        "\n",
        "    def format_data(features, labels):\n",
        "        labels = tf.reshape(labels, [-1, 1])\n",
        "        return features, labels\n",
        "\n",
        "    return dataset.map(format_data)\n",
        "\n",
        "def build_keras_model(hparams):\n",
        "    input_features = []\n",
        "    for feature in NUMERIC_FEATURES + CATEGORICAL_FEATURES:\n",
        "        input_features.append(tf.keras.Input(shape=(1,), name=transformed_name(feature)))\n",
        "\n",
        "    concatenate = tf.keras.layers.concatenate(input_features)\n",
        "\n",
        "    deep = tf.keras.layers.Dense(hparams['unit_1'], activation=\"relu\")(concatenate)\n",
        "    deep = tf.keras.layers.Dense(hparams['unit_2'], activation=\"relu\")(deep)\n",
        "    deep = tf.keras.layers.Dense(hparams['unit_3'], activation=\"relu\")(deep)\n",
        "    deep = tf.keras.layers.Dense(hparams['unit_4'], activation=\"relu\")(deep)\n",
        "\n",
        "    outputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(deep)\n",
        "\n",
        "    model = tf.keras.Model(inputs=input_features, outputs=outputs)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=hparams['learning_rate']),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=[tf.keras.metrics.BinaryAccuracy()]\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "# Fungsi untuk menyajikan TF examples\n",
        "def _get_serve_tf_examples_fn(model, tf_transform_output):\n",
        "    model.tft_layer = tf_transform_output.transform_features_layer()\n",
        "\n",
        "    @tf.function\n",
        "    def serve_tf_examples_fn(serialized_tf_examples):\n",
        "        feature_spec = tf_transform_output.raw_feature_spec()\n",
        "        feature_spec.pop(LABEL_KEY)\n",
        "\n",
        "        parsed_features = tf.io.parse_example(serialized_tf_examples, feature_spec)\n",
        "        transformed_features = model.tft_layer(parsed_features)\n",
        "\n",
        "        return model(transformed_features)\n",
        "\n",
        "    return serve_tf_examples_fn\n",
        "\n",
        "# Fungsi untuk mendapatkan signature dari fitur transformasi\n",
        "def _get_transform_features_signature(model, tf_transform_output):\n",
        "    model.tft_layer_eval = tf_transform_output.transform_features_layer()\n",
        "\n",
        "    @tf.function(input_signature=[\n",
        "        tf.TensorSpec(shape=[None], dtype=tf.string, name='examples')\n",
        "    ])\n",
        "    def transform_features_fn(serialized_tf_example):\n",
        "        raw_feature_spec = tf_transform_output.raw_feature_spec()\n",
        "        raw_features = tf.io.parse_example(serialized_tf_example, raw_feature_spec)\n",
        "        transformed_features = model.tft_layer_eval(raw_features)\n",
        "        return transformed_features\n",
        "\n",
        "    return transform_features_fn\n",
        "\n",
        "def run_fn(fn_args: FnArgs):\n",
        "    tf_transform_output = tft.TFTransformOutput(fn_args.transform_graph_path)\n",
        "\n",
        "    train_dataset = input_fn(fn_args.train_files, tf_transform_output)\n",
        "    eval_dataset = input_fn(fn_args.eval_files, tf_transform_output)\n",
        "\n",
        "    best_hyperparameters = fn_args.hyperparameters.get('values')\n",
        "\n",
        "    model = build_keras_model(best_hyperparameters)\n",
        "\n",
        "    model.fit(\n",
        "        train_dataset,\n",
        "        steps_per_epoch=fn_args.train_steps,\n",
        "        validation_data=eval_dataset,\n",
        "        validation_steps=fn_args.eval_steps,\n",
        "        epochs=10\n",
        "    )\n",
        "\n",
        "    signatures = {\n",
        "        'serving_default': _get_serve_tf_examples_fn(model, tf_transform_output).get_concrete_function(\n",
        "            tf.TensorSpec(shape=[None], dtype=tf.string, name='examples')),\n",
        "        'transform_features': _get_transform_features_signature(model, tf_transform_output),\n",
        "    }\n",
        "\n",
        "    tf.saved_model.save(model, fn_args.serving_model_dir, signatures=signatures)\n",
        "\n",
        "    plot_model(\n",
        "        model,\n",
        "        to_file='images/model_plot.png',\n",
        "        show_shapes=True,\n",
        "        show_layer_names=True\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnWGU4EhkrA8"
      },
      "source": [
        "# Melakukan Inisialisasi Local Pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "BTn5zKtDXolE"
      },
      "outputs": [],
      "source": [
        "def init_local_pipeline(\n",
        "    components, pipeline_root: Text\n",
        ") -> pipeline.Pipeline:\n",
        "\n",
        "    logging.info(f\"Pipeline root set to: {pipeline_root}\")\n",
        "    beam_args = [\n",
        "        \"--direct_running_mode=multi_processing\",\n",
        "        # 0 auto-detect based on on the number of CPUs available\n",
        "        # during execution time.\n",
        "        \"----direct_num_workers=0\"\n",
        "    ]\n",
        "\n",
        "    return pipeline.Pipeline(\n",
        "        pipeline_name=PIPELINE_NAME,\n",
        "        pipeline_root=pipeline_root,\n",
        "        components=components,\n",
        "        enable_cache=True,\n",
        "        metadata_connection_config=metadata.sqlite_metadata_connection_config(\n",
        "            metadata_path\n",
        "        ),\n",
        "        beam_pipeline_args=beam_args\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ahCZMwnkw7t"
      },
      "source": [
        "# Menjalankan Pipeline Menggunakan Apache Beam."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-4ID2udgXoh_",
        "outputId": "d86eba32-b633-4155-f4f3-5b328f64a0a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:absl:Finished tuning... Tuner ID: tuner0\n",
            "INFO:absl:Best HyperParameters: {'space': [{'class_name': 'Choice', 'config': {'name': 'unit_1', 'default': 8, 'conditions': [], 'values': [8, 16], 'ordered': True}}, {'class_name': 'Choice', 'config': {'name': 'unit_2', 'default': 16, 'conditions': [], 'values': [16, 32], 'ordered': True}}, {'class_name': 'Choice', 'config': {'name': 'unit_3', 'default': 16, 'conditions': [], 'values': [16, 32], 'ordered': True}}, {'class_name': 'Choice', 'config': {'name': 'unit_4', 'default': 32, 'conditions': [], 'values': [32, 64], 'ordered': True}}, {'class_name': 'Choice', 'config': {'name': 'learning_rate', 'default': 0.01, 'conditions': [], 'values': [0.01, 0.001], 'ordered': True}}], 'values': {'unit_1': 8, 'unit_2': 16, 'unit_3': 16, 'unit_4': 32, 'learning_rate': 0.01}}\n",
            "INFO:absl:Best Hyperparameters are written to output/heart-disease-pipeline/Tuner/best_hyperparameters/117/best_hyperparameters.txt.\n",
            "INFO:absl:Tuner results are written to output/heart-disease-pipeline/Tuner/tuner_results/117/tuner_results.json.\n",
            "INFO:absl:Cleaning up stateless execution info.\n",
            "INFO:absl:Execution 117 succeeded.\n",
            "INFO:absl:Cleaning up stateful execution info.\n",
            "INFO:absl:Deleted stateful_working_dir output/heart-disease-pipeline/Tuner/.system/stateful_working_dir/bcd69f2a-7f9a-4b14-a6a2-d0ced7e4f6fc\n",
            "INFO:absl:Publishing output artifacts defaultdict(<class 'list'>, {'best_hyperparameters': [Artifact(artifact: uri: \"output/heart-disease-pipeline/Tuner/best_hyperparameters/117\"\n",
            ", artifact_type: name: \"HyperParameters\"\n",
            ")], 'tuner_results': [Artifact(artifact: uri: \"output/heart-disease-pipeline/Tuner/tuner_results/117\"\n",
            ", artifact_type: name: \"TunerResults\"\n",
            ")]}) for execution 117\n",
            "INFO:absl:MetadataStore with DB connection initialized\n",
            "INFO:absl:node Tuner is finished.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 10 Complete [00h 00m 47s]\n",
            "val_binary_accuracy: 0.8435999751091003\n",
            "\n",
            "Best val_binary_accuracy So Far: 0.8632000088691711\n",
            "Total elapsed time: 00h 07m 05s\n",
            "Results summary\n",
            "Results in output/heart-disease-pipeline/Tuner/.system/executor_execution/117/.temp/117/tuner/kt_random_search\n",
            "Showing 10 best trials\n",
            "Objective(name=\"val_binary_accuracy\", direction=\"max\")\n",
            "\n",
            "Trial 01 summary\n",
            "Hyperparameters:\n",
            "unit_1: 8\n",
            "unit_2: 16\n",
            "unit_3: 16\n",
            "unit_4: 32\n",
            "learning_rate: 0.01\n",
            "Score: 0.8632000088691711\n",
            "\n",
            "Trial 07 summary\n",
            "Hyperparameters:\n",
            "unit_1: 8\n",
            "unit_2: 32\n",
            "unit_3: 16\n",
            "unit_4: 32\n",
            "learning_rate: 0.001\n",
            "Score: 0.8628000020980835\n",
            "\n",
            "Trial 08 summary\n",
            "Hyperparameters:\n",
            "unit_1: 16\n",
            "unit_2: 32\n",
            "unit_3: 16\n",
            "unit_4: 32\n",
            "learning_rate: 0.001\n",
            "Score: 0.8435999751091003\n",
            "\n",
            "Trial 09 summary\n",
            "Hyperparameters:\n",
            "unit_1: 16\n",
            "unit_2: 32\n",
            "unit_3: 16\n",
            "unit_4: 64\n",
            "learning_rate: 0.001\n",
            "Score: 0.8435999751091003\n",
            "\n",
            "Trial 04 summary\n",
            "Hyperparameters:\n",
            "unit_1: 16\n",
            "unit_2: 16\n",
            "unit_3: 32\n",
            "unit_4: 32\n",
            "learning_rate: 0.001\n",
            "Score: 0.8432000279426575\n",
            "\n",
            "Trial 02 summary\n",
            "Hyperparameters:\n",
            "unit_1: 16\n",
            "unit_2: 32\n",
            "unit_3: 16\n",
            "unit_4: 32\n",
            "learning_rate: 0.01\n",
            "Score: 0.8240000009536743\n",
            "\n",
            "Trial 05 summary\n",
            "Hyperparameters:\n",
            "unit_1: 16\n",
            "unit_2: 16\n",
            "unit_3: 16\n",
            "unit_4: 64\n",
            "learning_rate: 0.001\n",
            "Score: 0.8235999941825867\n",
            "\n",
            "Trial 03 summary\n",
            "Hyperparameters:\n",
            "unit_1: 16\n",
            "unit_2: 32\n",
            "unit_3: 32\n",
            "unit_4: 32\n",
            "learning_rate: 0.001\n",
            "Score: 0.823199987411499\n",
            "\n",
            "Trial 00 summary\n",
            "Hyperparameters:\n",
            "unit_1: 16\n",
            "unit_2: 32\n",
            "unit_3: 32\n",
            "unit_4: 64\n",
            "learning_rate: 0.001\n",
            "Score: 0.7843999862670898\n",
            "\n",
            "Trial 06 summary\n",
            "Hyperparameters:\n",
            "unit_1: 16\n",
            "unit_2: 16\n",
            "unit_3: 32\n",
            "unit_4: 64\n",
            "learning_rate: 0.01\n",
            "Score: 0.7648000121116638\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:absl:node Trainer is running.\n",
            "INFO:absl:Running launcher for node_info {\n",
            "  type {\n",
            "    name: \"tfx.components.trainer.component.Trainer\"\n",
            "    base_type: TRAIN\n",
            "  }\n",
            "  id: \"Trainer\"\n",
            "}\n",
            "contexts {\n",
            "  contexts {\n",
            "    type {\n",
            "      name: \"pipeline\"\n",
            "    }\n",
            "    name {\n",
            "      field_value {\n",
            "        string_value: \"heart-disease-pipeline\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  contexts {\n",
            "    type {\n",
            "      name: \"pipeline_run\"\n",
            "    }\n",
            "    name {\n",
            "      field_value {\n",
            "        string_value: \"20241225-163901.665857\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  contexts {\n",
            "    type {\n",
            "      name: \"node\"\n",
            "    }\n",
            "    name {\n",
            "      field_value {\n",
            "        string_value: \"heart-disease-pipeline.Trainer\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "inputs {\n",
            "  inputs {\n",
            "    key: \"examples\"\n",
            "    value {\n",
            "      channels {\n",
            "        producer_node_query {\n",
            "          id: \"Transform\"\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"pipeline\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"heart-disease-pipeline\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"pipeline_run\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"20241225-163901.665857\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"node\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"heart-disease-pipeline.Transform\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        artifact_query {\n",
            "          type {\n",
            "            name: \"Examples\"\n",
            "            base_type: DATASET\n",
            "          }\n",
            "        }\n",
            "        output_key: \"transformed_examples\"\n",
            "      }\n",
            "      min_count: 1\n",
            "    }\n",
            "  }\n",
            "  inputs {\n",
            "    key: \"hyperparameters\"\n",
            "    value {\n",
            "      channels {\n",
            "        producer_node_query {\n",
            "          id: \"Tuner\"\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"pipeline\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"heart-disease-pipeline\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"pipeline_run\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"20241225-163901.665857\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"node\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"heart-disease-pipeline.Tuner\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        artifact_query {\n",
            "          type {\n",
            "            name: \"HyperParameters\"\n",
            "          }\n",
            "        }\n",
            "        output_key: \"best_hyperparameters\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  inputs {\n",
            "    key: \"schema\"\n",
            "    value {\n",
            "      channels {\n",
            "        producer_node_query {\n",
            "          id: \"SchemaGen\"\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"pipeline\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"heart-disease-pipeline\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"pipeline_run\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"20241225-163901.665857\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"node\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"heart-disease-pipeline.SchemaGen\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        artifact_query {\n",
            "          type {\n",
            "            name: \"Schema\"\n",
            "          }\n",
            "        }\n",
            "        output_key: \"schema\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  inputs {\n",
            "    key: \"transform_graph\"\n",
            "    value {\n",
            "      channels {\n",
            "        producer_node_query {\n",
            "          id: \"Transform\"\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"pipeline\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"heart-disease-pipeline\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"pipeline_run\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"20241225-163901.665857\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"node\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"heart-disease-pipeline.Transform\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        artifact_query {\n",
            "          type {\n",
            "            name: \"TransformGraph\"\n",
            "          }\n",
            "        }\n",
            "        output_key: \"transform_graph\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "outputs {\n",
            "  outputs {\n",
            "    key: \"model\"\n",
            "    value {\n",
            "      artifact_spec {\n",
            "        type {\n",
            "          name: \"Model\"\n",
            "          base_type: MODEL\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  outputs {\n",
            "    key: \"model_run\"\n",
            "    value {\n",
            "      artifact_spec {\n",
            "        type {\n",
            "          name: \"ModelRun\"\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "parameters {\n",
            "  parameters {\n",
            "    key: \"custom_config\"\n",
            "    value {\n",
            "      field_value {\n",
            "        string_value: \"null\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  parameters {\n",
            "    key: \"eval_args\"\n",
            "    value {\n",
            "      field_value {\n",
            "        string_value: \"{\\n  \\\"num_steps\\\": 250,\\n  \\\"splits\\\": [\\n    \\\"eval\\\"\\n  ]\\n}\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  parameters {\n",
            "    key: \"module_path\"\n",
            "    value {\n",
            "      field_value {\n",
            "        string_value: \"trainer@output/heart-disease-pipeline/_wheels/tfx_user_code_Trainer-0.0+bf9253ca112a8b73a729c9ef8bb004df5fc991f1e828045ea61d3a9f10802261-py3-none-any.whl\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  parameters {\n",
            "    key: \"train_args\"\n",
            "    value {\n",
            "      field_value {\n",
            "        string_value: \"{\\n  \\\"num_steps\\\": 1000,\\n  \\\"splits\\\": [\\n    \\\"train\\\"\\n  ]\\n}\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "upstream_nodes: \"SchemaGen\"\n",
            "upstream_nodes: \"Transform\"\n",
            "upstream_nodes: \"Tuner\"\n",
            "downstream_nodes: \"Evaluator\"\n",
            "downstream_nodes: \"Pusher\"\n",
            "execution_options {\n",
            "  caching_options {\n",
            "    enable_cache: true\n",
            "  }\n",
            "}\n",
            "\n",
            "INFO:absl:MetadataStore with DB connection initialized\n",
            "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
            "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
            "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
            "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
            "INFO:absl:[Trainer] Resolved inputs: ({'schema': [Artifact(artifact: id: 3\n",
            "type_id: 20\n",
            "uri: \"output/heart-disease-pipeline/SchemaGen/schema/5\"\n",
            "custom_properties {\n",
            "  key: \"is_external\"\n",
            "  value {\n",
            "    int_value: 0\n",
            "  }\n",
            "}\n",
            "custom_properties {\n",
            "  key: \"tfx_version\"\n",
            "  value {\n",
            "    string_value: \"1.16.0\"\n",
            "  }\n",
            "}\n",
            "state: LIVE\n",
            "type: \"Schema\"\n",
            "create_time_since_epoch: 1735125073531\n",
            "last_update_time_since_epoch: 1735125073531\n",
            ", artifact_type: id: 20\n",
            "name: \"Schema\"\n",
            ")], 'transform_graph': [Artifact(artifact: id: 121\n",
            "type_id: 23\n",
            "uri: \"output/heart-disease-pipeline/Transform/transform_graph/116\"\n",
            "custom_properties {\n",
            "  key: \"is_external\"\n",
            "  value {\n",
            "    int_value: 0\n",
            "  }\n",
            "}\n",
            "custom_properties {\n",
            "  key: \"tfx_version\"\n",
            "  value {\n",
            "    string_value: \"1.16.0\"\n",
            "  }\n",
            "}\n",
            "state: LIVE\n",
            "type: \"TransformGraph\"\n",
            "create_time_since_epoch: 1735144814354\n",
            "last_update_time_since_epoch: 1735144814354\n",
            ", artifact_type: id: 23\n",
            "name: \"TransformGraph\"\n",
            ")], 'examples': [Artifact(artifact: id: 126\n",
            "type_id: 16\n",
            "uri: \"output/heart-disease-pipeline/Transform/transformed_examples/116\"\n",
            "properties {\n",
            "  key: \"split_names\"\n",
            "  value {\n",
            "    string_value: \"[\\\"train\\\", \\\"eval\\\"]\"\n",
            "  }\n",
            "}\n",
            "custom_properties {\n",
            "  key: \"is_external\"\n",
            "  value {\n",
            "    int_value: 0\n",
            "  }\n",
            "}\n",
            "custom_properties {\n",
            "  key: \"tfx_version\"\n",
            "  value {\n",
            "    string_value: \"1.16.0\"\n",
            "  }\n",
            "}\n",
            "state: LIVE\n",
            "type: \"Examples\"\n",
            "create_time_since_epoch: 1735144814355\n",
            "last_update_time_since_epoch: 1735144814355\n",
            ", artifact_type: id: 16\n",
            "name: \"Examples\"\n",
            "properties {\n",
            "  key: \"span\"\n",
            "  value: INT\n",
            "}\n",
            "properties {\n",
            "  key: \"split_names\"\n",
            "  value: STRING\n",
            "}\n",
            "properties {\n",
            "  key: \"version\"\n",
            "  value: INT\n",
            "}\n",
            "base_type: DATASET\n",
            ")], 'hyperparameters': [Artifact(artifact: id: 129\n",
            "type_id: 28\n",
            "uri: \"output/heart-disease-pipeline/Tuner/best_hyperparameters/117\"\n",
            "custom_properties {\n",
            "  key: \"is_external\"\n",
            "  value {\n",
            "    int_value: 0\n",
            "  }\n",
            "}\n",
            "custom_properties {\n",
            "  key: \"tfx_version\"\n",
            "  value {\n",
            "    string_value: \"1.16.0\"\n",
            "  }\n",
            "}\n",
            "state: LIVE\n",
            "type: \"HyperParameters\"\n",
            "create_time_since_epoch: 1735145247251\n",
            "last_update_time_since_epoch: 1735145247251\n",
            ", artifact_type: id: 28\n",
            "name: \"HyperParameters\"\n",
            ")]},)\n",
            "INFO:absl:MetadataStore with DB connection initialized\n",
            "INFO:absl:Going to run a new execution 118\n",
            "INFO:absl:Going to run a new execution: ExecutionInfo(execution_id=118, input_dict={'schema': [Artifact(artifact: id: 3\n",
            "type_id: 20\n",
            "uri: \"output/heart-disease-pipeline/SchemaGen/schema/5\"\n",
            "custom_properties {\n",
            "  key: \"is_external\"\n",
            "  value {\n",
            "    int_value: 0\n",
            "  }\n",
            "}\n",
            "custom_properties {\n",
            "  key: \"tfx_version\"\n",
            "  value {\n",
            "    string_value: \"1.16.0\"\n",
            "  }\n",
            "}\n",
            "state: LIVE\n",
            "type: \"Schema\"\n",
            "create_time_since_epoch: 1735125073531\n",
            "last_update_time_since_epoch: 1735125073531\n",
            ", artifact_type: id: 20\n",
            "name: \"Schema\"\n",
            ")], 'transform_graph': [Artifact(artifact: id: 121\n",
            "type_id: 23\n",
            "uri: \"output/heart-disease-pipeline/Transform/transform_graph/116\"\n",
            "custom_properties {\n",
            "  key: \"is_external\"\n",
            "  value {\n",
            "    int_value: 0\n",
            "  }\n",
            "}\n",
            "custom_properties {\n",
            "  key: \"tfx_version\"\n",
            "  value {\n",
            "    string_value: \"1.16.0\"\n",
            "  }\n",
            "}\n",
            "state: LIVE\n",
            "type: \"TransformGraph\"\n",
            "create_time_since_epoch: 1735144814354\n",
            "last_update_time_since_epoch: 1735144814354\n",
            ", artifact_type: id: 23\n",
            "name: \"TransformGraph\"\n",
            ")], 'examples': [Artifact(artifact: id: 126\n",
            "type_id: 16\n",
            "uri: \"output/heart-disease-pipeline/Transform/transformed_examples/116\"\n",
            "properties {\n",
            "  key: \"split_names\"\n",
            "  value {\n",
            "    string_value: \"[\\\"train\\\", \\\"eval\\\"]\"\n",
            "  }\n",
            "}\n",
            "custom_properties {\n",
            "  key: \"is_external\"\n",
            "  value {\n",
            "    int_value: 0\n",
            "  }\n",
            "}\n",
            "custom_properties {\n",
            "  key: \"tfx_version\"\n",
            "  value {\n",
            "    string_value: \"1.16.0\"\n",
            "  }\n",
            "}\n",
            "state: LIVE\n",
            "type: \"Examples\"\n",
            "create_time_since_epoch: 1735144814355\n",
            "last_update_time_since_epoch: 1735144814355\n",
            ", artifact_type: id: 16\n",
            "name: \"Examples\"\n",
            "properties {\n",
            "  key: \"span\"\n",
            "  value: INT\n",
            "}\n",
            "properties {\n",
            "  key: \"split_names\"\n",
            "  value: STRING\n",
            "}\n",
            "properties {\n",
            "  key: \"version\"\n",
            "  value: INT\n",
            "}\n",
            "base_type: DATASET\n",
            ")], 'hyperparameters': [Artifact(artifact: id: 129\n",
            "type_id: 28\n",
            "uri: \"output/heart-disease-pipeline/Tuner/best_hyperparameters/117\"\n",
            "custom_properties {\n",
            "  key: \"is_external\"\n",
            "  value {\n",
            "    int_value: 0\n",
            "  }\n",
            "}\n",
            "custom_properties {\n",
            "  key: \"tfx_version\"\n",
            "  value {\n",
            "    string_value: \"1.16.0\"\n",
            "  }\n",
            "}\n",
            "state: LIVE\n",
            "type: \"HyperParameters\"\n",
            "create_time_since_epoch: 1735145247251\n",
            "last_update_time_since_epoch: 1735145247251\n",
            ", artifact_type: id: 28\n",
            "name: \"HyperParameters\"\n",
            ")]}, output_dict=defaultdict(<class 'list'>, {'model_run': [Artifact(artifact: uri: \"output/heart-disease-pipeline/Trainer/model_run/118\"\n",
            ", artifact_type: name: \"ModelRun\"\n",
            ")], 'model': [Artifact(artifact: uri: \"output/heart-disease-pipeline/Trainer/model/118\"\n",
            ", artifact_type: name: \"Model\"\n",
            "base_type: MODEL\n",
            ")]}), exec_properties={'eval_args': '{\\n  \"num_steps\": 250,\\n  \"splits\": [\\n    \"eval\"\\n  ]\\n}', 'module_path': 'trainer@output/heart-disease-pipeline/_wheels/tfx_user_code_Trainer-0.0+bf9253ca112a8b73a729c9ef8bb004df5fc991f1e828045ea61d3a9f10802261-py3-none-any.whl', 'custom_config': 'null', 'train_args': '{\\n  \"num_steps\": 1000,\\n  \"splits\": [\\n    \"train\"\\n  ]\\n}'}, execution_output_uri='output/heart-disease-pipeline/Trainer/.system/executor_execution/118/executor_output.pb', stateful_working_dir='output/heart-disease-pipeline/Trainer/.system/stateful_working_dir/906b0f13-84a6-4b35-a3a1-30966d151452', tmp_dir='output/heart-disease-pipeline/Trainer/.system/executor_execution/118/.temp/', pipeline_node=node_info {\n",
            "  type {\n",
            "    name: \"tfx.components.trainer.component.Trainer\"\n",
            "    base_type: TRAIN\n",
            "  }\n",
            "  id: \"Trainer\"\n",
            "}\n",
            "contexts {\n",
            "  contexts {\n",
            "    type {\n",
            "      name: \"pipeline\"\n",
            "    }\n",
            "    name {\n",
            "      field_value {\n",
            "        string_value: \"heart-disease-pipeline\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  contexts {\n",
            "    type {\n",
            "      name: \"pipeline_run\"\n",
            "    }\n",
            "    name {\n",
            "      field_value {\n",
            "        string_value: \"20241225-163901.665857\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  contexts {\n",
            "    type {\n",
            "      name: \"node\"\n",
            "    }\n",
            "    name {\n",
            "      field_value {\n",
            "        string_value: \"heart-disease-pipeline.Trainer\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "inputs {\n",
            "  inputs {\n",
            "    key: \"examples\"\n",
            "    value {\n",
            "      channels {\n",
            "        producer_node_query {\n",
            "          id: \"Transform\"\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"pipeline\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"heart-disease-pipeline\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"pipeline_run\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"20241225-163901.665857\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"node\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"heart-disease-pipeline.Transform\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        artifact_query {\n",
            "          type {\n",
            "            name: \"Examples\"\n",
            "            base_type: DATASET\n",
            "          }\n",
            "        }\n",
            "        output_key: \"transformed_examples\"\n",
            "      }\n",
            "      min_count: 1\n",
            "    }\n",
            "  }\n",
            "  inputs {\n",
            "    key: \"hyperparameters\"\n",
            "    value {\n",
            "      channels {\n",
            "        producer_node_query {\n",
            "          id: \"Tuner\"\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"pipeline\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"heart-disease-pipeline\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"pipeline_run\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"20241225-163901.665857\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"node\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"heart-disease-pipeline.Tuner\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        artifact_query {\n",
            "          type {\n",
            "            name: \"HyperParameters\"\n",
            "          }\n",
            "        }\n",
            "        output_key: \"best_hyperparameters\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  inputs {\n",
            "    key: \"schema\"\n",
            "    value {\n",
            "      channels {\n",
            "        producer_node_query {\n",
            "          id: \"SchemaGen\"\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"pipeline\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"heart-disease-pipeline\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"pipeline_run\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"20241225-163901.665857\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"node\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"heart-disease-pipeline.SchemaGen\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        artifact_query {\n",
            "          type {\n",
            "            name: \"Schema\"\n",
            "          }\n",
            "        }\n",
            "        output_key: \"schema\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  inputs {\n",
            "    key: \"transform_graph\"\n",
            "    value {\n",
            "      channels {\n",
            "        producer_node_query {\n",
            "          id: \"Transform\"\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"pipeline\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"heart-disease-pipeline\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"pipeline_run\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"20241225-163901.665857\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"node\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"heart-disease-pipeline.Transform\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        artifact_query {\n",
            "          type {\n",
            "            name: \"TransformGraph\"\n",
            "          }\n",
            "        }\n",
            "        output_key: \"transform_graph\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "outputs {\n",
            "  outputs {\n",
            "    key: \"model\"\n",
            "    value {\n",
            "      artifact_spec {\n",
            "        type {\n",
            "          name: \"Model\"\n",
            "          base_type: MODEL\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  outputs {\n",
            "    key: \"model_run\"\n",
            "    value {\n",
            "      artifact_spec {\n",
            "        type {\n",
            "          name: \"ModelRun\"\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "parameters {\n",
            "  parameters {\n",
            "    key: \"custom_config\"\n",
            "    value {\n",
            "      field_value {\n",
            "        string_value: \"null\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  parameters {\n",
            "    key: \"eval_args\"\n",
            "    value {\n",
            "      field_value {\n",
            "        string_value: \"{\\n  \\\"num_steps\\\": 250,\\n  \\\"splits\\\": [\\n    \\\"eval\\\"\\n  ]\\n}\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  parameters {\n",
            "    key: \"module_path\"\n",
            "    value {\n",
            "      field_value {\n",
            "        string_value: \"trainer@output/heart-disease-pipeline/_wheels/tfx_user_code_Trainer-0.0+bf9253ca112a8b73a729c9ef8bb004df5fc991f1e828045ea61d3a9f10802261-py3-none-any.whl\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  parameters {\n",
            "    key: \"train_args\"\n",
            "    value {\n",
            "      field_value {\n",
            "        string_value: \"{\\n  \\\"num_steps\\\": 1000,\\n  \\\"splits\\\": [\\n    \\\"train\\\"\\n  ]\\n}\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "upstream_nodes: \"SchemaGen\"\n",
            "upstream_nodes: \"Transform\"\n",
            "upstream_nodes: \"Tuner\"\n",
            "downstream_nodes: \"Evaluator\"\n",
            "downstream_nodes: \"Pusher\"\n",
            "execution_options {\n",
            "  caching_options {\n",
            "    enable_cache: true\n",
            "  }\n",
            "}\n",
            ", pipeline_info=id: \"heart-disease-pipeline\"\n",
            ", pipeline_run_id='20241225-163901.665857', top_level_pipeline_run_id=None, frontend_url=None)\n",
            "WARNING:absl:Examples artifact does not have payload_format custom property. Falling back to FORMAT_TF_EXAMPLE\n",
            "WARNING:absl:Examples artifact does not have payload_format custom property. Falling back to FORMAT_TF_EXAMPLE\n",
            "WARNING:absl:Examples artifact does not have payload_format custom property. Falling back to FORMAT_TF_EXAMPLE\n",
            "INFO:absl:udf_utils.get_fn {'eval_args': '{\\n  \"num_steps\": 250,\\n  \"splits\": [\\n    \"eval\"\\n  ]\\n}', 'module_path': 'trainer@output/heart-disease-pipeline/_wheels/tfx_user_code_Trainer-0.0+bf9253ca112a8b73a729c9ef8bb004df5fc991f1e828045ea61d3a9f10802261-py3-none-any.whl', 'custom_config': 'null', 'train_args': '{\\n  \"num_steps\": 1000,\\n  \"splits\": [\\n    \"train\"\\n  ]\\n}'} 'run_fn'\n",
            "INFO:absl:Installing 'output/heart-disease-pipeline/_wheels/tfx_user_code_Trainer-0.0+bf9253ca112a8b73a729c9ef8bb004df5fc991f1e828045ea61d3a9f10802261-py3-none-any.whl' to a temporary directory.\n",
            "INFO:absl:Executing: ['/usr/bin/python3', '-m', 'pip', 'install', '--target', '/tmp/tmprhdlnv6_', 'output/heart-disease-pipeline/_wheels/tfx_user_code_Trainer-0.0+bf9253ca112a8b73a729c9ef8bb004df5fc991f1e828045ea61d3a9f10802261-py3-none-any.whl']\n",
            "INFO:absl:Successfully installed 'output/heart-disease-pipeline/_wheels/tfx_user_code_Trainer-0.0+bf9253ca112a8b73a729c9ef8bb004df5fc991f1e828045ea61d3a9f10802261-py3-none-any.whl'.\n",
            "INFO:absl:Training model.\n",
            "INFO:absl:Feature age_xf has a shape . Setting to DenseTensor.\n",
            "INFO:absl:Feature ca_xf has a shape . Setting to DenseTensor.\n",
            "INFO:absl:Feature chol_xf has a shape . Setting to DenseTensor.\n",
            "INFO:absl:Feature cp_xf has a shape . Setting to DenseTensor.\n",
            "INFO:absl:Feature exang_xf has a shape . Setting to DenseTensor.\n",
            "INFO:absl:Feature fbs_xf has a shape . Setting to DenseTensor.\n",
            "INFO:absl:Feature oldpeak_xf has a shape . Setting to DenseTensor.\n",
            "INFO:absl:Feature restecg_xf has a shape . Setting to DenseTensor.\n",
            "INFO:absl:Feature sex_xf has a shape . Setting to DenseTensor.\n",
            "INFO:absl:Feature slope_xf has a shape . Setting to DenseTensor.\n",
            "INFO:absl:Feature target has a shape . Setting to DenseTensor.\n",
            "INFO:absl:Feature thal_xf has a shape . Setting to DenseTensor.\n",
            "INFO:absl:Feature thalach_xf has a shape . Setting to DenseTensor.\n",
            "INFO:absl:Feature trestbps_xf has a shape . Setting to DenseTensor.\n",
            "INFO:absl:Feature age_xf has a shape . Setting to DenseTensor.\n",
            "INFO:absl:Feature ca_xf has a shape . Setting to DenseTensor.\n",
            "INFO:absl:Feature chol_xf has a shape . Setting to DenseTensor.\n",
            "INFO:absl:Feature cp_xf has a shape . Setting to DenseTensor.\n",
            "INFO:absl:Feature exang_xf has a shape . Setting to DenseTensor.\n",
            "INFO:absl:Feature fbs_xf has a shape . Setting to DenseTensor.\n",
            "INFO:absl:Feature oldpeak_xf has a shape . Setting to DenseTensor.\n",
            "INFO:absl:Feature restecg_xf has a shape . Setting to DenseTensor.\n",
            "INFO:absl:Feature sex_xf has a shape . Setting to DenseTensor.\n",
            "INFO:absl:Feature slope_xf has a shape . Setting to DenseTensor.\n",
            "INFO:absl:Feature target has a shape . Setting to DenseTensor.\n",
            "INFO:absl:Feature thal_xf has a shape . Setting to DenseTensor.\n",
            "INFO:absl:Feature thalach_xf has a shape . Setting to DenseTensor.\n",
            "INFO:absl:Feature trestbps_xf has a shape . Setting to DenseTensor.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - binary_accuracy: 0.9315 - loss: 0.1610 - val_binary_accuracy: 0.8235 - val_loss: 3.8471\n",
            "Epoch 2/10\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - binary_accuracy: 0.9830 - loss: 0.0623 - val_binary_accuracy: 0.8627 - val_loss: 4.4017\n",
            "Epoch 3/10\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - binary_accuracy: 0.9998 - loss: 0.0011 - val_binary_accuracy: 0.8432 - val_loss: 5.6661\n",
            "Epoch 4/10\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - binary_accuracy: 1.0000 - loss: 7.8000e-05 - val_binary_accuracy: 0.8431 - val_loss: 6.2677\n",
            "Epoch 5/10\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - binary_accuracy: 1.0000 - loss: 2.4606e-05 - val_binary_accuracy: 0.8431 - val_loss: 6.6948\n",
            "Epoch 6/10\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - binary_accuracy: 1.0000 - loss: 1.0512e-05 - val_binary_accuracy: 0.8432 - val_loss: 7.0325\n",
            "Epoch 7/10\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - binary_accuracy: 1.0000 - loss: 4.9662e-06 - val_binary_accuracy: 0.8431 - val_loss: 7.3663\n",
            "Epoch 8/10\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - binary_accuracy: 1.0000 - loss: 2.4356e-06 - val_binary_accuracy: 0.8236 - val_loss: 7.6547\n",
            "Epoch 9/10\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - binary_accuracy: 1.0000 - loss: 1.2802e-06 - val_binary_accuracy: 0.8233 - val_loss: 7.9530\n",
            "Epoch 10/10\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - binary_accuracy: 1.0000 - loss: 6.8709e-07 - val_binary_accuracy: 0.8239 - val_loss: 8.1933\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:absl:Feature age has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "INFO:absl:Feature ca has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "INFO:absl:Feature chol has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "INFO:absl:Feature cp has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "INFO:absl:Feature exang has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "INFO:absl:Feature fbs has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "INFO:absl:Feature oldpeak has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "INFO:absl:Feature restecg has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "INFO:absl:Feature sex has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "INFO:absl:Feature slope has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "INFO:absl:Feature target has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "INFO:absl:Feature thal has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "INFO:absl:Feature thalach has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "INFO:absl:Feature trestbps has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "INFO:absl:Function `serve_tf_examples_fn` contains input name(s) 380549, 380559, 380569, 380579, 380589, 380599, 380609, resource with unsupported characters which will be renamed to transform_features_layer_380549, transform_features_layer_380559, transform_features_layer_380569, transform_features_layer_380579, transform_features_layer_380589, transform_features_layer_380599, transform_features_layer_380609, functional_1_1_dense_9_1_add_readvariableop_resource in the SavedModel.\n",
            "INFO:absl:Feature age has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "INFO:absl:Feature ca has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "INFO:absl:Feature chol has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "INFO:absl:Feature cp has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "INFO:absl:Feature exang has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "INFO:absl:Feature fbs has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "INFO:absl:Feature oldpeak has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "INFO:absl:Feature restecg has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "INFO:absl:Feature sex has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "INFO:absl:Feature slope has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "INFO:absl:Feature target has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "INFO:absl:Feature thal has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "INFO:absl:Feature thalach has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "INFO:absl:Feature trestbps has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "INFO:absl:Function `transform_features_fn` contains input name(s) 380883, 380893, 380903, 380913, 380923, 380933, 380943 with unsupported characters which will be renamed to transform_features_layer_380883, transform_features_layer_380893, transform_features_layer_380903, transform_features_layer_380913, transform_features_layer_380923, transform_features_layer_380933, transform_features_layer_380943 in the SavedModel.\n",
            "INFO:absl:Sharding callback duration: 33\n",
            "INFO:absl:Sharding callback duration: 45\n",
            "INFO:absl:Writing fingerprint to output/heart-disease-pipeline/Trainer/model/118/Format-Serving/fingerprint.pb\n",
            "INFO:absl:Training complete. Model written to output/heart-disease-pipeline/Trainer/model/118/Format-Serving. ModelRun written to output/heart-disease-pipeline/Trainer/model_run/118\n",
            "INFO:absl:Cleaning up stateless execution info.\n",
            "INFO:absl:Execution 118 succeeded.\n",
            "INFO:absl:Cleaning up stateful execution info.\n",
            "INFO:absl:Deleted stateful_working_dir output/heart-disease-pipeline/Trainer/.system/stateful_working_dir/906b0f13-84a6-4b35-a3a1-30966d151452\n",
            "INFO:absl:Publishing output artifacts defaultdict(<class 'list'>, {'model_run': [Artifact(artifact: uri: \"output/heart-disease-pipeline/Trainer/model_run/118\"\n",
            ", artifact_type: name: \"ModelRun\"\n",
            ")], 'model': [Artifact(artifact: uri: \"output/heart-disease-pipeline/Trainer/model/118\"\n",
            ", artifact_type: name: \"Model\"\n",
            "base_type: MODEL\n",
            ")]}) for execution 118\n",
            "INFO:absl:MetadataStore with DB connection initialized\n",
            "INFO:absl:node Trainer is finished.\n",
            "INFO:absl:node Evaluator is running.\n",
            "INFO:absl:Running launcher for node_info {\n",
            "  type {\n",
            "    name: \"tfx.components.evaluator.component.Evaluator\"\n",
            "    base_type: EVALUATE\n",
            "  }\n",
            "  id: \"Evaluator\"\n",
            "}\n",
            "contexts {\n",
            "  contexts {\n",
            "    type {\n",
            "      name: \"pipeline\"\n",
            "    }\n",
            "    name {\n",
            "      field_value {\n",
            "        string_value: \"heart-disease-pipeline\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  contexts {\n",
            "    type {\n",
            "      name: \"pipeline_run\"\n",
            "    }\n",
            "    name {\n",
            "      field_value {\n",
            "        string_value: \"20241225-163901.665857\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  contexts {\n",
            "    type {\n",
            "      name: \"node\"\n",
            "    }\n",
            "    name {\n",
            "      field_value {\n",
            "        string_value: \"heart-disease-pipeline.Evaluator\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "inputs {\n",
            "  inputs {\n",
            "    key: \"baseline_model\"\n",
            "    value {\n",
            "      channels {\n",
            "        producer_node_query {\n",
            "          id: \"Latest_blessed_model_resolver\"\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"pipeline\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"heart-disease-pipeline\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"pipeline_run\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"20241225-163901.665857\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"node\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"heart-disease-pipeline.Latest_blessed_model_resolver\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        artifact_query {\n",
            "          type {\n",
            "            name: \"Model\"\n",
            "            base_type: MODEL\n",
            "          }\n",
            "        }\n",
            "        output_key: \"model\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  inputs {\n",
            "    key: \"examples\"\n",
            "    value {\n",
            "      channels {\n",
            "        producer_node_query {\n",
            "          id: \"CsvExampleGen\"\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"pipeline\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"heart-disease-pipeline\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"pipeline_run\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"20241225-163901.665857\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"node\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"heart-disease-pipeline.CsvExampleGen\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        artifact_query {\n",
            "          type {\n",
            "            name: \"Examples\"\n",
            "            base_type: DATASET\n",
            "          }\n",
            "        }\n",
            "        output_key: \"examples\"\n",
            "      }\n",
            "      min_count: 1\n",
            "    }\n",
            "  }\n",
            "  inputs {\n",
            "    key: \"model\"\n",
            "    value {\n",
            "      channels {\n",
            "        producer_node_query {\n",
            "          id: \"Trainer\"\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"pipeline\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"heart-disease-pipeline\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"pipeline_run\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"20241225-163901.665857\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"node\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"heart-disease-pipeline.Trainer\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        artifact_query {\n",
            "          type {\n",
            "            name: \"Model\"\n",
            "            base_type: MODEL\n",
            "          }\n",
            "        }\n",
            "        output_key: \"model\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "outputs {\n",
            "  outputs {\n",
            "    key: \"blessing\"\n",
            "    value {\n",
            "      artifact_spec {\n",
            "        type {\n",
            "          name: \"ModelBlessing\"\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  outputs {\n",
            "    key: \"evaluation\"\n",
            "    value {\n",
            "      artifact_spec {\n",
            "        type {\n",
            "          name: \"ModelEvaluation\"\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "parameters {\n",
            "  parameters {\n",
            "    key: \"eval_config\"\n",
            "    value {\n",
            "      field_value {\n",
            "        string_value: \"{\\n  \\\"metrics_specs\\\": [\\n    {\\n      \\\"metrics\\\": [\\n        {\\n          \\\"class_name\\\": \\\"AUC\\\"\\n        },\\n        {\\n          \\\"class_name\\\": \\\"Precision\\\"\\n        },\\n        {\\n          \\\"class_name\\\": \\\"Recall\\\"\\n        },\\n        {\\n          \\\"class_name\\\": \\\"ExampleCount\\\"\\n        },\\n        {\\n          \\\"class_name\\\": \\\"BinaryAccuracy\\\",\\n          \\\"threshold\\\": {\\n            \\\"change_threshold\\\": {\\n              \\\"absolute\\\": 0.0001,\\n              \\\"direction\\\": \\\"HIGHER_IS_BETTER\\\"\\n            },\\n            \\\"value_threshold\\\": {\\n              \\\"lower_bound\\\": 0.8\\n            }\\n          }\\n        }\\n      ]\\n    }\\n  ],\\n  \\\"model_specs\\\": [\\n    {\\n      \\\"label_key\\\": \\\"target\\\"\\n    }\\n  ],\\n  \\\"slicing_specs\\\": [\\n    {}\\n  ]\\n}\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  parameters {\n",
            "    key: \"example_splits\"\n",
            "    value {\n",
            "      field_value {\n",
            "        string_value: \"null\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  parameters {\n",
            "    key: \"fairness_indicator_thresholds\"\n",
            "    value {\n",
            "      field_value {\n",
            "        string_value: \"null\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "upstream_nodes: \"CsvExampleGen\"\n",
            "upstream_nodes: \"Latest_blessed_model_resolver\"\n",
            "upstream_nodes: \"Trainer\"\n",
            "downstream_nodes: \"Pusher\"\n",
            "execution_options {\n",
            "  caching_options {\n",
            "    enable_cache: true\n",
            "  }\n",
            "}\n",
            "\n",
            "INFO:absl:MetadataStore with DB connection initialized\n",
            "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
            "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
            "INFO:absl:[Evaluator] Resolved inputs: ({'examples': [Artifact(artifact: id: 1\n",
            "type_id: 16\n",
            "uri: \"output/heart-disease-pipeline/CsvExampleGen/examples/3\"\n",
            "properties {\n",
            "  key: \"split_names\"\n",
            "  value {\n",
            "    string_value: \"[\\\"train\\\", \\\"eval\\\"]\"\n",
            "  }\n",
            "}\n",
            "custom_properties {\n",
            "  key: \"file_format\"\n",
            "  value {\n",
            "    string_value: \"tfrecords_gzip\"\n",
            "  }\n",
            "}\n",
            "custom_properties {\n",
            "  key: \"input_fingerprint\"\n",
            "  value {\n",
            "    string_value: \"split:single_split,num_files:1,total_bytes:11021,xor_checksum:1735125005,sum_checksum:1735125005\"\n",
            "  }\n",
            "}\n",
            "custom_properties {\n",
            "  key: \"is_external\"\n",
            "  value {\n",
            "    int_value: 0\n",
            "  }\n",
            "}\n",
            "custom_properties {\n",
            "  key: \"payload_format\"\n",
            "  value {\n",
            "    string_value: \"FORMAT_TF_EXAMPLE\"\n",
            "  }\n",
            "}\n",
            "custom_properties {\n",
            "  key: \"span\"\n",
            "  value {\n",
            "    int_value: 0\n",
            "  }\n",
            "}\n",
            "custom_properties {\n",
            "  key: \"tfx_version\"\n",
            "  value {\n",
            "    string_value: \"1.16.0\"\n",
            "  }\n",
            "}\n",
            "state: LIVE\n",
            "type: \"Examples\"\n",
            "create_time_since_epoch: 1735125058907\n",
            "last_update_time_since_epoch: 1735125058907\n",
            ", artifact_type: id: 16\n",
            "name: \"Examples\"\n",
            "properties {\n",
            "  key: \"span\"\n",
            "  value: INT\n",
            "}\n",
            "properties {\n",
            "  key: \"split_names\"\n",
            "  value: STRING\n",
            "}\n",
            "properties {\n",
            "  key: \"version\"\n",
            "  value: INT\n",
            "}\n",
            "base_type: DATASET\n",
            ")], 'baseline_model': [], 'model': [Artifact(artifact: id: 132\n",
            "type_id: 31\n",
            "uri: \"output/heart-disease-pipeline/Trainer/model/118\"\n",
            "custom_properties {\n",
            "  key: \"is_external\"\n",
            "  value {\n",
            "    int_value: 0\n",
            "  }\n",
            "}\n",
            "custom_properties {\n",
            "  key: \"tfx_version\"\n",
            "  value {\n",
            "    string_value: \"1.16.0\"\n",
            "  }\n",
            "}\n",
            "state: LIVE\n",
            "type: \"Model\"\n",
            "create_time_since_epoch: 1735145349774\n",
            "last_update_time_since_epoch: 1735145349774\n",
            ", artifact_type: id: 31\n",
            "name: \"Model\"\n",
            "base_type: MODEL\n",
            ")]},)\n",
            "INFO:absl:MetadataStore with DB connection initialized\n",
            "INFO:absl:Going to run a new execution 119\n",
            "INFO:absl:Going to run a new execution: ExecutionInfo(execution_id=119, input_dict={'examples': [Artifact(artifact: id: 1\n",
            "type_id: 16\n",
            "uri: \"output/heart-disease-pipeline/CsvExampleGen/examples/3\"\n",
            "properties {\n",
            "  key: \"split_names\"\n",
            "  value {\n",
            "    string_value: \"[\\\"train\\\", \\\"eval\\\"]\"\n",
            "  }\n",
            "}\n",
            "custom_properties {\n",
            "  key: \"file_format\"\n",
            "  value {\n",
            "    string_value: \"tfrecords_gzip\"\n",
            "  }\n",
            "}\n",
            "custom_properties {\n",
            "  key: \"input_fingerprint\"\n",
            "  value {\n",
            "    string_value: \"split:single_split,num_files:1,total_bytes:11021,xor_checksum:1735125005,sum_checksum:1735125005\"\n",
            "  }\n",
            "}\n",
            "custom_properties {\n",
            "  key: \"is_external\"\n",
            "  value {\n",
            "    int_value: 0\n",
            "  }\n",
            "}\n",
            "custom_properties {\n",
            "  key: \"payload_format\"\n",
            "  value {\n",
            "    string_value: \"FORMAT_TF_EXAMPLE\"\n",
            "  }\n",
            "}\n",
            "custom_properties {\n",
            "  key: \"span\"\n",
            "  value {\n",
            "    int_value: 0\n",
            "  }\n",
            "}\n",
            "custom_properties {\n",
            "  key: \"tfx_version\"\n",
            "  value {\n",
            "    string_value: \"1.16.0\"\n",
            "  }\n",
            "}\n",
            "state: LIVE\n",
            "type: \"Examples\"\n",
            "create_time_since_epoch: 1735125058907\n",
            "last_update_time_since_epoch: 1735125058907\n",
            ", artifact_type: id: 16\n",
            "name: \"Examples\"\n",
            "properties {\n",
            "  key: \"span\"\n",
            "  value: INT\n",
            "}\n",
            "properties {\n",
            "  key: \"split_names\"\n",
            "  value: STRING\n",
            "}\n",
            "properties {\n",
            "  key: \"version\"\n",
            "  value: INT\n",
            "}\n",
            "base_type: DATASET\n",
            ")], 'baseline_model': [], 'model': [Artifact(artifact: id: 132\n",
            "type_id: 31\n",
            "uri: \"output/heart-disease-pipeline/Trainer/model/118\"\n",
            "custom_properties {\n",
            "  key: \"is_external\"\n",
            "  value {\n",
            "    int_value: 0\n",
            "  }\n",
            "}\n",
            "custom_properties {\n",
            "  key: \"tfx_version\"\n",
            "  value {\n",
            "    string_value: \"1.16.0\"\n",
            "  }\n",
            "}\n",
            "state: LIVE\n",
            "type: \"Model\"\n",
            "create_time_since_epoch: 1735145349774\n",
            "last_update_time_since_epoch: 1735145349774\n",
            ", artifact_type: id: 31\n",
            "name: \"Model\"\n",
            "base_type: MODEL\n",
            ")]}, output_dict=defaultdict(<class 'list'>, {'blessing': [Artifact(artifact: uri: \"output/heart-disease-pipeline/Evaluator/blessing/119\"\n",
            ", artifact_type: name: \"ModelBlessing\"\n",
            ")], 'evaluation': [Artifact(artifact: uri: \"output/heart-disease-pipeline/Evaluator/evaluation/119\"\n",
            ", artifact_type: name: \"ModelEvaluation\"\n",
            ")]}), exec_properties={'eval_config': '{\\n  \"metrics_specs\": [\\n    {\\n      \"metrics\": [\\n        {\\n          \"class_name\": \"AUC\"\\n        },\\n        {\\n          \"class_name\": \"Precision\"\\n        },\\n        {\\n          \"class_name\": \"Recall\"\\n        },\\n        {\\n          \"class_name\": \"ExampleCount\"\\n        },\\n        {\\n          \"class_name\": \"BinaryAccuracy\",\\n          \"threshold\": {\\n            \"change_threshold\": {\\n              \"absolute\": 0.0001,\\n              \"direction\": \"HIGHER_IS_BETTER\"\\n            },\\n            \"value_threshold\": {\\n              \"lower_bound\": 0.8\\n            }\\n          }\\n        }\\n      ]\\n    }\\n  ],\\n  \"model_specs\": [\\n    {\\n      \"label_key\": \"target\"\\n    }\\n  ],\\n  \"slicing_specs\": [\\n    {}\\n  ]\\n}', 'example_splits': 'null', 'fairness_indicator_thresholds': 'null'}, execution_output_uri='output/heart-disease-pipeline/Evaluator/.system/executor_execution/119/executor_output.pb', stateful_working_dir='output/heart-disease-pipeline/Evaluator/.system/stateful_working_dir/34991abc-9b97-40d3-afc9-82c02cf5dc69', tmp_dir='output/heart-disease-pipeline/Evaluator/.system/executor_execution/119/.temp/', pipeline_node=node_info {\n",
            "  type {\n",
            "    name: \"tfx.components.evaluator.component.Evaluator\"\n",
            "    base_type: EVALUATE\n",
            "  }\n",
            "  id: \"Evaluator\"\n",
            "}\n",
            "contexts {\n",
            "  contexts {\n",
            "    type {\n",
            "      name: \"pipeline\"\n",
            "    }\n",
            "    name {\n",
            "      field_value {\n",
            "        string_value: \"heart-disease-pipeline\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  contexts {\n",
            "    type {\n",
            "      name: \"pipeline_run\"\n",
            "    }\n",
            "    name {\n",
            "      field_value {\n",
            "        string_value: \"20241225-163901.665857\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  contexts {\n",
            "    type {\n",
            "      name: \"node\"\n",
            "    }\n",
            "    name {\n",
            "      field_value {\n",
            "        string_value: \"heart-disease-pipeline.Evaluator\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "inputs {\n",
            "  inputs {\n",
            "    key: \"baseline_model\"\n",
            "    value {\n",
            "      channels {\n",
            "        producer_node_query {\n",
            "          id: \"Latest_blessed_model_resolver\"\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"pipeline\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"heart-disease-pipeline\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"pipeline_run\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"20241225-163901.665857\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"node\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"heart-disease-pipeline.Latest_blessed_model_resolver\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        artifact_query {\n",
            "          type {\n",
            "            name: \"Model\"\n",
            "            base_type: MODEL\n",
            "          }\n",
            "        }\n",
            "        output_key: \"model\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  inputs {\n",
            "    key: \"examples\"\n",
            "    value {\n",
            "      channels {\n",
            "        producer_node_query {\n",
            "          id: \"CsvExampleGen\"\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"pipeline\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"heart-disease-pipeline\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"pipeline_run\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"20241225-163901.665857\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"node\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"heart-disease-pipeline.CsvExampleGen\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        artifact_query {\n",
            "          type {\n",
            "            name: \"Examples\"\n",
            "            base_type: DATASET\n",
            "          }\n",
            "        }\n",
            "        output_key: \"examples\"\n",
            "      }\n",
            "      min_count: 1\n",
            "    }\n",
            "  }\n",
            "  inputs {\n",
            "    key: \"model\"\n",
            "    value {\n",
            "      channels {\n",
            "        producer_node_query {\n",
            "          id: \"Trainer\"\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"pipeline\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"heart-disease-pipeline\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"pipeline_run\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"20241225-163901.665857\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"node\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"heart-disease-pipeline.Trainer\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        artifact_query {\n",
            "          type {\n",
            "            name: \"Model\"\n",
            "            base_type: MODEL\n",
            "          }\n",
            "        }\n",
            "        output_key: \"model\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "outputs {\n",
            "  outputs {\n",
            "    key: \"blessing\"\n",
            "    value {\n",
            "      artifact_spec {\n",
            "        type {\n",
            "          name: \"ModelBlessing\"\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  outputs {\n",
            "    key: \"evaluation\"\n",
            "    value {\n",
            "      artifact_spec {\n",
            "        type {\n",
            "          name: \"ModelEvaluation\"\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "parameters {\n",
            "  parameters {\n",
            "    key: \"eval_config\"\n",
            "    value {\n",
            "      field_value {\n",
            "        string_value: \"{\\n  \\\"metrics_specs\\\": [\\n    {\\n      \\\"metrics\\\": [\\n        {\\n          \\\"class_name\\\": \\\"AUC\\\"\\n        },\\n        {\\n          \\\"class_name\\\": \\\"Precision\\\"\\n        },\\n        {\\n          \\\"class_name\\\": \\\"Recall\\\"\\n        },\\n        {\\n          \\\"class_name\\\": \\\"ExampleCount\\\"\\n        },\\n        {\\n          \\\"class_name\\\": \\\"BinaryAccuracy\\\",\\n          \\\"threshold\\\": {\\n            \\\"change_threshold\\\": {\\n              \\\"absolute\\\": 0.0001,\\n              \\\"direction\\\": \\\"HIGHER_IS_BETTER\\\"\\n            },\\n            \\\"value_threshold\\\": {\\n              \\\"lower_bound\\\": 0.8\\n            }\\n          }\\n        }\\n      ]\\n    }\\n  ],\\n  \\\"model_specs\\\": [\\n    {\\n      \\\"label_key\\\": \\\"target\\\"\\n    }\\n  ],\\n  \\\"slicing_specs\\\": [\\n    {}\\n  ]\\n}\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  parameters {\n",
            "    key: \"example_splits\"\n",
            "    value {\n",
            "      field_value {\n",
            "        string_value: \"null\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  parameters {\n",
            "    key: \"fairness_indicator_thresholds\"\n",
            "    value {\n",
            "      field_value {\n",
            "        string_value: \"null\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "upstream_nodes: \"CsvExampleGen\"\n",
            "upstream_nodes: \"Latest_blessed_model_resolver\"\n",
            "upstream_nodes: \"Trainer\"\n",
            "downstream_nodes: \"Pusher\"\n",
            "execution_options {\n",
            "  caching_options {\n",
            "    enable_cache: true\n",
            "  }\n",
            "}\n",
            ", pipeline_info=id: \"heart-disease-pipeline\"\n",
            ", pipeline_run_id='20241225-163901.665857', top_level_pipeline_run_id=None, frontend_url=None)\n",
            "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['----direct_num_workers=0']\n",
            "INFO:absl:Attempting to infer TFX Python dependency for beam\n",
            "INFO:absl:Copying all content from install dir /usr/local/lib/python3.10/dist-packages/tfx to temp dir /tmp/tmpp5m6kelh/build/tfx\n",
            "INFO:absl:Generating a temp setup file at /tmp/tmpp5m6kelh/build/tfx/setup.py\n",
            "INFO:absl:Creating temporary sdist package, logs available at /tmp/tmpp5m6kelh/build/tfx/setup.log\n",
            "INFO:absl:Added --extra_package=/tmp/tmpp5m6kelh/build/tfx/dist/tfx_ephemeral-1.16.0.tar.gz to beam args\n",
            "INFO:absl:udf_utils.get_fn {'eval_config': '{\\n  \"metrics_specs\": [\\n    {\\n      \"metrics\": [\\n        {\\n          \"class_name\": \"AUC\"\\n        },\\n        {\\n          \"class_name\": \"Precision\"\\n        },\\n        {\\n          \"class_name\": \"Recall\"\\n        },\\n        {\\n          \"class_name\": \"ExampleCount\"\\n        },\\n        {\\n          \"class_name\": \"BinaryAccuracy\",\\n          \"threshold\": {\\n            \"change_threshold\": {\\n              \"absolute\": 0.0001,\\n              \"direction\": \"HIGHER_IS_BETTER\"\\n            },\\n            \"value_threshold\": {\\n              \"lower_bound\": 0.8\\n            }\\n          }\\n        }\\n      ]\\n    }\\n  ],\\n  \"model_specs\": [\\n    {\\n      \"label_key\": \"target\"\\n    }\\n  ],\\n  \"slicing_specs\": [\\n    {}\\n  ]\\n}', 'example_splits': 'null', 'fairness_indicator_thresholds': 'null'} 'custom_eval_shared_model'\n",
            "INFO:absl:Request was made to ignore the baseline ModelSpec and any change thresholds. This is likely because a baseline model was not provided: updated_config=\n",
            "model_specs {\n",
            "  label_key: \"target\"\n",
            "}\n",
            "slicing_specs {\n",
            "}\n",
            "metrics_specs {\n",
            "  metrics {\n",
            "    class_name: \"AUC\"\n",
            "  }\n",
            "  metrics {\n",
            "    class_name: \"Precision\"\n",
            "  }\n",
            "  metrics {\n",
            "    class_name: \"Recall\"\n",
            "  }\n",
            "  metrics {\n",
            "    class_name: \"ExampleCount\"\n",
            "  }\n",
            "  metrics {\n",
            "    class_name: \"BinaryAccuracy\"\n",
            "    threshold {\n",
            "      value_threshold {\n",
            "        lower_bound {\n",
            "          value: 0.8\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n",
            "INFO:absl:Using output/heart-disease-pipeline/Trainer/model/118/Format-Serving as  model.\n",
            "INFO:absl:The 'example_splits' parameter is not set, using 'eval' split.\n",
            "INFO:absl:Evaluating model.\n",
            "INFO:absl:udf_utils.get_fn {'eval_config': '{\\n  \"metrics_specs\": [\\n    {\\n      \"metrics\": [\\n        {\\n          \"class_name\": \"AUC\"\\n        },\\n        {\\n          \"class_name\": \"Precision\"\\n        },\\n        {\\n          \"class_name\": \"Recall\"\\n        },\\n        {\\n          \"class_name\": \"ExampleCount\"\\n        },\\n        {\\n          \"class_name\": \"BinaryAccuracy\",\\n          \"threshold\": {\\n            \"change_threshold\": {\\n              \"absolute\": 0.0001,\\n              \"direction\": \"HIGHER_IS_BETTER\"\\n            },\\n            \"value_threshold\": {\\n              \"lower_bound\": 0.8\\n            }\\n          }\\n        }\\n      ]\\n    }\\n  ],\\n  \"model_specs\": [\\n    {\\n      \"label_key\": \"target\"\\n    }\\n  ],\\n  \"slicing_specs\": [\\n    {}\\n  ]\\n}', 'example_splits': 'null', 'fairness_indicator_thresholds': 'null'} 'custom_extractors'\n",
            "INFO:absl:Request was made to ignore the baseline ModelSpec and any change thresholds. This is likely because a baseline model was not provided: updated_config=\n",
            "model_specs {\n",
            "  label_key: \"target\"\n",
            "}\n",
            "slicing_specs {\n",
            "}\n",
            "metrics_specs {\n",
            "  metrics {\n",
            "    class_name: \"AUC\"\n",
            "  }\n",
            "  metrics {\n",
            "    class_name: \"Precision\"\n",
            "  }\n",
            "  metrics {\n",
            "    class_name: \"Recall\"\n",
            "  }\n",
            "  metrics {\n",
            "    class_name: \"ExampleCount\"\n",
            "  }\n",
            "  metrics {\n",
            "    class_name: \"BinaryAccuracy\"\n",
            "    threshold {\n",
            "      value_threshold {\n",
            "        lower_bound {\n",
            "          value: 0.8\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  model_names: \"\"\n",
            "}\n",
            "\n",
            "INFO:absl:Request was made to ignore the baseline ModelSpec and any change thresholds. This is likely because a baseline model was not provided: updated_config=\n",
            "model_specs {\n",
            "  label_key: \"target\"\n",
            "}\n",
            "slicing_specs {\n",
            "}\n",
            "metrics_specs {\n",
            "  metrics {\n",
            "    class_name: \"AUC\"\n",
            "  }\n",
            "  metrics {\n",
            "    class_name: \"Precision\"\n",
            "  }\n",
            "  metrics {\n",
            "    class_name: \"Recall\"\n",
            "  }\n",
            "  metrics {\n",
            "    class_name: \"ExampleCount\"\n",
            "  }\n",
            "  metrics {\n",
            "    class_name: \"BinaryAccuracy\"\n",
            "    threshold {\n",
            "      value_threshold {\n",
            "        lower_bound {\n",
            "          value: 0.8\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  model_names: \"\"\n",
            "}\n",
            "\n",
            "INFO:absl:eval_shared_models have model_types: {'tf_generic'}\n",
            "INFO:absl:Request was made to ignore the baseline ModelSpec and any change thresholds. This is likely because a baseline model was not provided: updated_config=\n",
            "model_specs {\n",
            "  label_key: \"target\"\n",
            "}\n",
            "slicing_specs {\n",
            "}\n",
            "metrics_specs {\n",
            "  metrics {\n",
            "    class_name: \"AUC\"\n",
            "  }\n",
            "  metrics {\n",
            "    class_name: \"Precision\"\n",
            "  }\n",
            "  metrics {\n",
            "    class_name: \"Recall\"\n",
            "  }\n",
            "  metrics {\n",
            "    class_name: \"ExampleCount\"\n",
            "  }\n",
            "  metrics {\n",
            "    class_name: \"BinaryAccuracy\"\n",
            "    threshold {\n",
            "      value_threshold {\n",
            "        lower_bound {\n",
            "          value: 0.8\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  model_names: \"\"\n",
            "}\n",
            "\n",
            "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['----direct_num_workers=0']\n",
            "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1735145358   nanos: 184114933 } message: \"No semi_persistent_directory found: Functions defined in __main__ (interactive session) may fail.\" log_location: \"/usr/local/lib/python3.10/dist-packages/apache_beam/runners/worker/sdk_worker_main.py:361\" thread: \"MainThread\" \n",
            "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1735145358   nanos: 192946672 } message: \"Discarding unparseable args: [\\'--pipeline_type_check\\', \\'--direct_runner_use_stacked_bundle\\']\" log_location: \"/usr/local/lib/python3.10/dist-packages/apache_beam/options/pipeline_options.py:394\" thread: \"MainThread\" \n",
            "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1735145363   nanos: 675748348 } message: \"Couldn\\'t find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\" instruction_id: \"bundle_248\" transform_id: \"ReadFromTFRecordToArrow[eval][0]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process\" log_location: \"/usr/local/lib/python3.10/dist-packages/apache_beam/io/tfrecordio.py:59\" thread: \"Thread-13\" \n",
            "INFO:absl:Evaluation complete. Results written to output/heart-disease-pipeline/Evaluator/evaluation/119.\n",
            "INFO:absl:Checking validation results.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow_model_analysis/writers/metrics_plots_and_validations_writer.py:112: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use eager execution and: \n",
            "`tf.data.TFRecordDataset(path)`\n",
            "INFO:absl:Blessing result True written to output/heart-disease-pipeline/Evaluator/blessing/119.\n",
            "INFO:absl:Cleaning up stateless execution info.\n",
            "INFO:absl:Execution 119 succeeded.\n",
            "INFO:absl:Cleaning up stateful execution info.\n",
            "INFO:absl:Deleted stateful_working_dir output/heart-disease-pipeline/Evaluator/.system/stateful_working_dir/34991abc-9b97-40d3-afc9-82c02cf5dc69\n",
            "INFO:absl:Publishing output artifacts defaultdict(<class 'list'>, {'blessing': [Artifact(artifact: uri: \"output/heart-disease-pipeline/Evaluator/blessing/119\"\n",
            ", artifact_type: name: \"ModelBlessing\"\n",
            ")], 'evaluation': [Artifact(artifact: uri: \"output/heart-disease-pipeline/Evaluator/evaluation/119\"\n",
            ", artifact_type: name: \"ModelEvaluation\"\n",
            ")]}) for execution 119\n",
            "INFO:absl:MetadataStore with DB connection initialized\n",
            "INFO:absl:node Evaluator is finished.\n",
            "INFO:absl:node Pusher is running.\n",
            "INFO:absl:Running launcher for node_info {\n",
            "  type {\n",
            "    name: \"tfx.components.pusher.component.Pusher\"\n",
            "    base_type: DEPLOY\n",
            "  }\n",
            "  id: \"Pusher\"\n",
            "}\n",
            "contexts {\n",
            "  contexts {\n",
            "    type {\n",
            "      name: \"pipeline\"\n",
            "    }\n",
            "    name {\n",
            "      field_value {\n",
            "        string_value: \"heart-disease-pipeline\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  contexts {\n",
            "    type {\n",
            "      name: \"pipeline_run\"\n",
            "    }\n",
            "    name {\n",
            "      field_value {\n",
            "        string_value: \"20241225-163901.665857\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  contexts {\n",
            "    type {\n",
            "      name: \"node\"\n",
            "    }\n",
            "    name {\n",
            "      field_value {\n",
            "        string_value: \"heart-disease-pipeline.Pusher\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "inputs {\n",
            "  inputs {\n",
            "    key: \"model\"\n",
            "    value {\n",
            "      channels {\n",
            "        producer_node_query {\n",
            "          id: \"Trainer\"\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"pipeline\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"heart-disease-pipeline\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"pipeline_run\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"20241225-163901.665857\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"node\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"heart-disease-pipeline.Trainer\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        artifact_query {\n",
            "          type {\n",
            "            name: \"Model\"\n",
            "            base_type: MODEL\n",
            "          }\n",
            "        }\n",
            "        output_key: \"model\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  inputs {\n",
            "    key: \"model_blessing\"\n",
            "    value {\n",
            "      channels {\n",
            "        producer_node_query {\n",
            "          id: \"Evaluator\"\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"pipeline\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"heart-disease-pipeline\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"pipeline_run\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"20241225-163901.665857\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"node\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"heart-disease-pipeline.Evaluator\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        artifact_query {\n",
            "          type {\n",
            "            name: \"ModelBlessing\"\n",
            "          }\n",
            "        }\n",
            "        output_key: \"blessing\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "outputs {\n",
            "  outputs {\n",
            "    key: \"pushed_model\"\n",
            "    value {\n",
            "      artifact_spec {\n",
            "        type {\n",
            "          name: \"PushedModel\"\n",
            "          base_type: MODEL\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "parameters {\n",
            "  parameters {\n",
            "    key: \"custom_config\"\n",
            "    value {\n",
            "      field_value {\n",
            "        string_value: \"null\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  parameters {\n",
            "    key: \"push_destination\"\n",
            "    value {\n",
            "      field_value {\n",
            "        string_value: \"{\\n  \\\"filesystem\\\": {\\n    \\\"base_directory\\\": \\\"output/serving_model\\\"\\n  }\\n}\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "upstream_nodes: \"Evaluator\"\n",
            "upstream_nodes: \"Trainer\"\n",
            "execution_options {\n",
            "  caching_options {\n",
            "    enable_cache: true\n",
            "  }\n",
            "}\n",
            "\n",
            "INFO:absl:MetadataStore with DB connection initialized\n",
            "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
            "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
            "INFO:absl:[Pusher] Resolved inputs: ({'model': [Artifact(artifact: id: 132\n",
            "type_id: 31\n",
            "uri: \"output/heart-disease-pipeline/Trainer/model/118\"\n",
            "custom_properties {\n",
            "  key: \"is_external\"\n",
            "  value {\n",
            "    int_value: 0\n",
            "  }\n",
            "}\n",
            "custom_properties {\n",
            "  key: \"tfx_version\"\n",
            "  value {\n",
            "    string_value: \"1.16.0\"\n",
            "  }\n",
            "}\n",
            "state: LIVE\n",
            "type: \"Model\"\n",
            "create_time_since_epoch: 1735145349774\n",
            "last_update_time_since_epoch: 1735145349774\n",
            ", artifact_type: id: 31\n",
            "name: \"Model\"\n",
            "base_type: MODEL\n",
            ")], 'model_blessing': [Artifact(artifact: id: 133\n",
            "type_id: 33\n",
            "uri: \"output/heart-disease-pipeline/Evaluator/blessing/119\"\n",
            "custom_properties {\n",
            "  key: \"blessed\"\n",
            "  value {\n",
            "    int_value: 1\n",
            "  }\n",
            "}\n",
            "custom_properties {\n",
            "  key: \"current_model\"\n",
            "  value {\n",
            "    string_value: \"output/heart-disease-pipeline/Trainer/model/118\"\n",
            "  }\n",
            "}\n",
            "custom_properties {\n",
            "  key: \"current_model_id\"\n",
            "  value {\n",
            "    int_value: 132\n",
            "  }\n",
            "}\n",
            "custom_properties {\n",
            "  key: \"is_external\"\n",
            "  value {\n",
            "    int_value: 0\n",
            "  }\n",
            "}\n",
            "custom_properties {\n",
            "  key: \"tfx_version\"\n",
            "  value {\n",
            "    string_value: \"1.16.0\"\n",
            "  }\n",
            "}\n",
            "state: LIVE\n",
            "type: \"ModelBlessing\"\n",
            "create_time_since_epoch: 1735145366750\n",
            "last_update_time_since_epoch: 1735145366750\n",
            ", artifact_type: id: 33\n",
            "name: \"ModelBlessing\"\n",
            ")]},)\n",
            "INFO:absl:MetadataStore with DB connection initialized\n",
            "INFO:absl:Going to run a new execution 120\n",
            "INFO:absl:Going to run a new execution: ExecutionInfo(execution_id=120, input_dict={'model': [Artifact(artifact: id: 132\n",
            "type_id: 31\n",
            "uri: \"output/heart-disease-pipeline/Trainer/model/118\"\n",
            "custom_properties {\n",
            "  key: \"is_external\"\n",
            "  value {\n",
            "    int_value: 0\n",
            "  }\n",
            "}\n",
            "custom_properties {\n",
            "  key: \"tfx_version\"\n",
            "  value {\n",
            "    string_value: \"1.16.0\"\n",
            "  }\n",
            "}\n",
            "state: LIVE\n",
            "type: \"Model\"\n",
            "create_time_since_epoch: 1735145349774\n",
            "last_update_time_since_epoch: 1735145349774\n",
            ", artifact_type: id: 31\n",
            "name: \"Model\"\n",
            "base_type: MODEL\n",
            ")], 'model_blessing': [Artifact(artifact: id: 133\n",
            "type_id: 33\n",
            "uri: \"output/heart-disease-pipeline/Evaluator/blessing/119\"\n",
            "custom_properties {\n",
            "  key: \"blessed\"\n",
            "  value {\n",
            "    int_value: 1\n",
            "  }\n",
            "}\n",
            "custom_properties {\n",
            "  key: \"current_model\"\n",
            "  value {\n",
            "    string_value: \"output/heart-disease-pipeline/Trainer/model/118\"\n",
            "  }\n",
            "}\n",
            "custom_properties {\n",
            "  key: \"current_model_id\"\n",
            "  value {\n",
            "    int_value: 132\n",
            "  }\n",
            "}\n",
            "custom_properties {\n",
            "  key: \"is_external\"\n",
            "  value {\n",
            "    int_value: 0\n",
            "  }\n",
            "}\n",
            "custom_properties {\n",
            "  key: \"tfx_version\"\n",
            "  value {\n",
            "    string_value: \"1.16.0\"\n",
            "  }\n",
            "}\n",
            "state: LIVE\n",
            "type: \"ModelBlessing\"\n",
            "create_time_since_epoch: 1735145366750\n",
            "last_update_time_since_epoch: 1735145366750\n",
            ", artifact_type: id: 33\n",
            "name: \"ModelBlessing\"\n",
            ")]}, output_dict=defaultdict(<class 'list'>, {'pushed_model': [Artifact(artifact: uri: \"output/heart-disease-pipeline/Pusher/pushed_model/120\"\n",
            ", artifact_type: name: \"PushedModel\"\n",
            "base_type: MODEL\n",
            ")]}), exec_properties={'push_destination': '{\\n  \"filesystem\": {\\n    \"base_directory\": \"output/serving_model\"\\n  }\\n}', 'custom_config': 'null'}, execution_output_uri='output/heart-disease-pipeline/Pusher/.system/executor_execution/120/executor_output.pb', stateful_working_dir='output/heart-disease-pipeline/Pusher/.system/stateful_working_dir/9566a301-0708-478f-8614-7778cf59e654', tmp_dir='output/heart-disease-pipeline/Pusher/.system/executor_execution/120/.temp/', pipeline_node=node_info {\n",
            "  type {\n",
            "    name: \"tfx.components.pusher.component.Pusher\"\n",
            "    base_type: DEPLOY\n",
            "  }\n",
            "  id: \"Pusher\"\n",
            "}\n",
            "contexts {\n",
            "  contexts {\n",
            "    type {\n",
            "      name: \"pipeline\"\n",
            "    }\n",
            "    name {\n",
            "      field_value {\n",
            "        string_value: \"heart-disease-pipeline\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  contexts {\n",
            "    type {\n",
            "      name: \"pipeline_run\"\n",
            "    }\n",
            "    name {\n",
            "      field_value {\n",
            "        string_value: \"20241225-163901.665857\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  contexts {\n",
            "    type {\n",
            "      name: \"node\"\n",
            "    }\n",
            "    name {\n",
            "      field_value {\n",
            "        string_value: \"heart-disease-pipeline.Pusher\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "inputs {\n",
            "  inputs {\n",
            "    key: \"model\"\n",
            "    value {\n",
            "      channels {\n",
            "        producer_node_query {\n",
            "          id: \"Trainer\"\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"pipeline\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"heart-disease-pipeline\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"pipeline_run\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"20241225-163901.665857\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"node\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"heart-disease-pipeline.Trainer\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        artifact_query {\n",
            "          type {\n",
            "            name: \"Model\"\n",
            "            base_type: MODEL\n",
            "          }\n",
            "        }\n",
            "        output_key: \"model\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  inputs {\n",
            "    key: \"model_blessing\"\n",
            "    value {\n",
            "      channels {\n",
            "        producer_node_query {\n",
            "          id: \"Evaluator\"\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"pipeline\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"heart-disease-pipeline\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"pipeline_run\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"20241225-163901.665857\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"node\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"heart-disease-pipeline.Evaluator\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        artifact_query {\n",
            "          type {\n",
            "            name: \"ModelBlessing\"\n",
            "          }\n",
            "        }\n",
            "        output_key: \"blessing\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "outputs {\n",
            "  outputs {\n",
            "    key: \"pushed_model\"\n",
            "    value {\n",
            "      artifact_spec {\n",
            "        type {\n",
            "          name: \"PushedModel\"\n",
            "          base_type: MODEL\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "parameters {\n",
            "  parameters {\n",
            "    key: \"custom_config\"\n",
            "    value {\n",
            "      field_value {\n",
            "        string_value: \"null\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  parameters {\n",
            "    key: \"push_destination\"\n",
            "    value {\n",
            "      field_value {\n",
            "        string_value: \"{\\n  \\\"filesystem\\\": {\\n    \\\"base_directory\\\": \\\"output/serving_model\\\"\\n  }\\n}\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "upstream_nodes: \"Evaluator\"\n",
            "upstream_nodes: \"Trainer\"\n",
            "execution_options {\n",
            "  caching_options {\n",
            "    enable_cache: true\n",
            "  }\n",
            "}\n",
            ", pipeline_info=id: \"heart-disease-pipeline\"\n",
            ", pipeline_run_id='20241225-163901.665857', top_level_pipeline_run_id=None, frontend_url=None)\n",
            "INFO:absl:Model version: 1735145367\n",
            "INFO:absl:Model written to serving path output/serving_model/1735145367.\n",
            "INFO:absl:Model pushed to output/heart-disease-pipeline/Pusher/pushed_model/120.\n",
            "INFO:absl:Cleaning up stateless execution info.\n",
            "INFO:absl:Execution 120 succeeded.\n",
            "INFO:absl:Cleaning up stateful execution info.\n",
            "INFO:absl:Deleted stateful_working_dir output/heart-disease-pipeline/Pusher/.system/stateful_working_dir/9566a301-0708-478f-8614-7778cf59e654\n",
            "INFO:absl:Publishing output artifacts defaultdict(<class 'list'>, {'pushed_model': [Artifact(artifact: uri: \"output/heart-disease-pipeline/Pusher/pushed_model/120\"\n",
            ", artifact_type: name: \"PushedModel\"\n",
            "base_type: MODEL\n",
            ")]}) for execution 120\n",
            "INFO:absl:MetadataStore with DB connection initialized\n",
            "INFO:absl:node Pusher is finished.\n"
          ]
        }
      ],
      "source": [
        "from modules.components import init_components\n",
        "\n",
        "logging.set_verbosity(logging.INFO)\n",
        "\n",
        "config = {\n",
        "    \"DATA_ROOT\": DATA_ROOT,\n",
        "    \"tuner_module\": TUNER_MODULE_FILE,\n",
        "    \"training_module\": TRAINER_MODULE_FILE,\n",
        "    \"transform_module\": TRANSFORM_MODULE_FILE,\n",
        "    \"training_steps\": 1000,\n",
        "    \"eval_steps\": 250,\n",
        "    \"serving_model_dir\": serving_model_dir,\n",
        "}\n",
        "\n",
        "components = init_components(config)\n",
        "\n",
        "pipeline = init_local_pipeline(components, pipeline_root)\n",
        "BeamDagRunner().run(pipeline=pipeline)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G69N2IMxXofQ",
        "outputId": "722c0f25-2f91-4ab7-ab91-e004339defc4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dockerfile has been written.\n"
          ]
        }
      ],
      "source": [
        "# Tulis isi Dockerfile\n",
        "dockerfile_content = \"\"\"\n",
        "FROM tensorflow/serving:latest\n",
        "\n",
        "COPY ./output/serving_model /models/cc-model\n",
        "COPY ./config /model_config\n",
        "ENV MODEL_NAME=cc-model\n",
        "\n",
        "ENV MONITORING_CONFIG=\"/model_config/prometheus.config\"\n",
        "ENV PORT=8501\n",
        "RUN echo '#!/bin/bash \\n\\n\\\n",
        "env \\n\\\n",
        "tensorflow_model_server --port=8500 --rest_api_port=${PORT} \\\n",
        "--model_name=${MODEL_NAME} --model_base_path=${MODEL_BASE_PATH}/${MODEL_NAME} \\\n",
        "--monitoring_config_file=${MONITORING_CONFIG} \\\n",
        "\"$@\"' > /usr/bin/tf_serving_entrypoint.sh \\\n",
        "&& chmod +x /usr/bin/tf_serving_entrypoint.sh\n",
        "\"\"\"\n",
        "\n",
        "# Simpan sebagai file Dockerfile\n",
        "with open(\"Dockerfile\", \"w\") as f:\n",
        "    f.write(dockerfile_content)\n",
        "\n",
        "print(\"Dockerfile has been written.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9jl_k7cpiMLA",
        "outputId": "465b51d4-a733-467e-c21f-b2a62bd39b55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dockerfile has been written.\n"
          ]
        }
      ],
      "source": [
        "# Tulis isi Dockerfile\n",
        "dockerfile_content = \"\"\"\n",
        "FROM prom/prometheus:latest\n",
        "\n",
        "COPY prometheus.yml /etc/prometheus/prometheus.yml\n",
        "\"\"\"\n",
        "\n",
        "# Simpan sebagai file Dockerfile\n",
        "with open(\"/content/monitoring/Dockerfile\", \"w\") as f:\n",
        "    f.write(dockerfile_content)\n",
        "\n",
        "print(\"Dockerfile has been written.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "aub-5p68iXFc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e475e17-2050-41a5-a334-08b891ff9b25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prometheus has been written.\n"
          ]
        }
      ],
      "source": [
        "# Tulis isi yml\n",
        "yml_content = \"\"\"\n",
        "global:\n",
        "  scrape_interval: 5s\n",
        "  evaluation_interval: 5s\n",
        "  external_labels:\n",
        "    monitor: \"tf-serving-monitor\"\n",
        "\n",
        "scrape_configs:\n",
        "  - job_name: \"prometheus\"\n",
        "    scrape_interval: 5s\n",
        "    metrics_path: /monitoring/prometheus/metrics\n",
        "    scheme: \"https\"\n",
        "    static_configs:\n",
        "      - targets: ['proyek-akhir-mlops-production.up.railway.app']\n",
        "\"\"\"\n",
        "\n",
        "# Simpan sebagai file Dockerfile\n",
        "with open(\"/content/monitoring/prometheus.yml\", \"w\") as f:\n",
        "    f.write(yml_content)\n",
        "\n",
        "print(\"prometheus has been written.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "SzdkKVNWiscS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ce003ef-ecae-4703-96df-309aecf211f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prometheus has been written.\n"
          ]
        }
      ],
      "source": [
        "# Tulis isi config\n",
        "config_content = \"\"\"\n",
        "prometheus_config {\n",
        "   enable: true,\n",
        "   path: \"/monitoring/prometheus/metrics\"\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# Simpan sebagai file Dockerfile\n",
        "with open(\"/content/config/prometheus.config\", \"w\") as f:\n",
        "    f.write(config_content)\n",
        "\n",
        "print(\"prometheus has been written.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "A7zq9PbqhTVU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8590ed12-e0cb-4e53-f230-1afbbccf7c0d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/data.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "import shutil\n",
        "\n",
        "# Path to the directory you want to zip\n",
        "dir_path = '/content'\n",
        "\n",
        "# Path where the zip file will be saved\n",
        "output_zip_path = '/content/data.zip'\n",
        "\n",
        "# Create a zip file\n",
        "shutil.make_archive(output_zip_path.replace('.zip', ''), 'zip', dir_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVS35GpZy0NQ"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "mGwLlSBcVpn7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "034ed284-da21-4355-c8ad-1ef392d35d7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Label: Positive\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import json\n",
        "import requests\n",
        "import base64\n",
        "\n",
        "# Fungsi untuk membuat serialized example\n",
        "def create_serialized_example(inputs):\n",
        "    feature_mapping = {\n",
        "        \"age\": tf.train.Feature(int64_list=tf.train.Int64List(value=[int(inputs[\"age\"])])),\n",
        "        \"sex\": tf.train.Feature(int64_list=tf.train.Int64List(value=[int(inputs[\"sex\"])])),\n",
        "        \"cp\": tf.train.Feature(int64_list=tf.train.Int64List(value=[int(inputs[\"cp\"])])),\n",
        "        \"trestbps\": tf.train.Feature(int64_list=tf.train.Int64List(value=[int(inputs[\"trestbps\"])])),\n",
        "        \"chol\": tf.train.Feature(int64_list=tf.train.Int64List(value=[int(inputs[\"chol\"])])),\n",
        "        \"fbs\": tf.train.Feature(int64_list=tf.train.Int64List(value=[int(inputs[\"fbs\"])])),\n",
        "        \"restecg\": tf.train.Feature(int64_list=tf.train.Int64List(value=[int(inputs[\"restecg\"])])),\n",
        "        \"thalach\": tf.train.Feature(int64_list=tf.train.Int64List(value=[int(inputs[\"thalach\"])])),\n",
        "        \"exang\": tf.train.Feature(int64_list=tf.train.Int64List(value=[int(inputs[\"exang\"])])),\n",
        "        \"oldpeak\": tf.train.Feature(float_list=tf.train.FloatList(value=[float(inputs[\"oldpeak\"])])),\n",
        "        \"slope\": tf.train.Feature(int64_list=tf.train.Int64List(value=[int(inputs[\"slope\"])])),\n",
        "        \"ca\": tf.train.Feature(int64_list=tf.train.Int64List(value=[int(inputs[\"ca\"])])),\n",
        "        \"thal\": tf.train.Feature(int64_list=tf.train.Int64List(value=[int(inputs[\"thal\"])])),\n",
        "    }\n",
        "\n",
        "    # Membuat tf.train.Example\n",
        "    example = tf.train.Example(features=tf.train.Features(feature=feature_mapping))\n",
        "    return example.SerializeToString()\n",
        "\n",
        "# URL endpoint TensorFlow Serving\n",
        "url = \"https://tugas-akhir-mlops-production.up.railway.app/v1/models/cc-model:predict\"\n",
        "\n",
        "# Data input\n",
        "inputs = {\n",
        "    \"age\": 63,\n",
        "    \"sex\": 1,\n",
        "    \"cp\": 3,\n",
        "    \"trestbps\": 145,\n",
        "    \"chol\": 233,\n",
        "    \"fbs\": 1,\n",
        "    \"restecg\": 0,\n",
        "    \"thalach\": 150,\n",
        "    \"exang\": 0,\n",
        "    \"oldpeak\": 2.3,\n",
        "    \"slope\": 0,\n",
        "    \"ca\": 0,\n",
        "    \"thal\": 1,\n",
        "}\n",
        "\n",
        "# Membuat serialized example\n",
        "serialized_example = create_serialized_example(inputs)\n",
        "\n",
        "# Mengonversi serialized example ke base64\n",
        "serialized_example_base64 = base64.b64encode(serialized_example).decode('utf-8')\n",
        "\n",
        "# Membuat payload untuk request\n",
        "data = {\n",
        "    \"signature_name\": \"serving_default\",  # Sesuaikan dengan signature pada model\n",
        "    \"instances\": [{\"b64\": serialized_example_base64}],  # Mengirim serialized example dalam format base64\n",
        "}\n",
        "\n",
        "# Mengirimkan request POST ke server\n",
        "response = requests.post(url, json=data)\n",
        "\n",
        "# Menangani response\n",
        "if response.status_code == 200:\n",
        "    predictions = response.json()\n",
        "    predicted_label = \"Positive\" if predictions[\"predictions\"][0][0] > 0.5 else \"Negative\"\n",
        "    print(f\"Predicted Label: {predicted_label}\")\n",
        "else:\n",
        "    print(f\"Error: {response.status_code} - {response.text}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8cA7qCVNxuha"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}